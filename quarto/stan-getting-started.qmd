---
title: "Getting Started with Stan"
subtitle: "in Python with CmdStanPy and plotnine"
author: "Bob Carpenter"
date: "March 27, 2023"
execute:
  cache: false
format:
  html:
    highlight-style: arrow
    mainfont: "Palatino"
    monofont: "Menlo, Lucida Console"
    fontsize: 14pt
    linestretch: 1.5
    number-sections: true
    number-depth: 3
    toc: true
    toc-location: right
    code-fold: true
    css: style.css
  pdf:
    number-sections: true
    number-depth: 3
    shift-heading-level-by: -1
    margin-bottom: 1in
    fig-pos: "t!"
    biblio-title: "References"
    biblio-style: natbib
    link-citations: true
    link-bibliography: true
    pdf-engine: xelatex
bibliography: references.bib
---

# Introduction

These notes are intended to introduce several technical topics to
practitioners: Bayesian statistics and probabilistic modeling, Markov
chain Monte Carlo methods for Bayesian inference, and the Stan
probabilistic programming language.


## Bayesian statistics

The general problem addressed by statistical inference is that of
reasoning from a limited number of noisy observations. For example, we
might want to perform inference about a population after measuring a
subset of its members, or we might want to predict future events after
observing past events.

There are many ways to go about applied statistics.  These notes focus
on Bayesian statistics, a form of statistical modeling and inference
that is grounded in probability theory.  In the Bayesian
apporach to statistics, we characterize our knowledge of the world in
terms of probabilities (e.g., there is a 24.3% chance of rain after
lunch today, the probability that the next baby born in the United
states is male is 51\%).

Bayesian inference is always carried out with respect to a
mathematical model of a stochastic data generating process. If the
model is well-specified in the sense of matching the true data
generating process, then Bayesian statistical inference can be shown
to have several desirable properties, such as calibration and
resistance to overfitting.

## Pre-requisites

I will assume the reader is familiar with the basic notions of
differential and integral calculus in multiple dimensions (e.g., the
typical first- and second-year undergraduate sequence).  But don't
worry, we only need calculus to define what we are computing---the
actual calculations will all be done by Stan.  I will further assume
that the reader is familiar with the basics of matrix operations like
multiplication, inversion, and determinants (e.g., as taught in an
intro to linear algebra class).  If you are not familiar with any of
this math, this is a good opportunity to brush up on or learn some
matrix algebra.

I will further assume that the reader is familiar with basic notions
of probability theory, including discrete and continuous densities,
probability density and mass functions, cumulative distribution
functions, and the basic rules of probability theory (e.g., as taught
in an introduction to mathematical statistics).

I will also assume the reader is familiar with basic Python numerical
programming, including NumPy and SciPy (there are a lot of tutorials
available online and in book form).  I'll be using [Python
3](https://www.python.org/downloads/) with the
[CmdStanPy](https://mc-stan.org/cmdstanpy/installation.html) interface
to Stan.  Click the links and follow the install instructions if you
would like to follow along in code.

## Python boilerplate

We include the following Python boilerplate to import and configure
packages we will use throughout these notes.

```{python}
# PROJECT SETUP
import cmdstanpy as csp
import numpy as np
import pandas as pd
import plotnine as pn
import itertools
import logging
import warnings

csp.utils.get_logger().setLevel(logging.WARNING)

warnings.filterwarnings( "ignore", module = "plotnine\..*" )

def mydraw(x):
    x.draw()
```

# Stan for forward simulation

We're first going to consider simple binomial sampling in order to
introduce Stan programs and how they're called and to develop some
intuitions about estimation based on binary outcomes.

Let's say we have 100 trials, each with a 30% chance of success. The
30% might represent a chance of rain and the result the number of days
out of 100 that it actually rained; it might represent the chance of a
drug improving a patient's condition with the result being the number
of patients who improve; or, it might represent the chance of a
successful penalty kick from a given position on a field, and the the
total number representing the number of goals in 100 attempts.

In statistical sampling notation, we write
$$
Y \sim \textrm{binomial}(N, \theta)
$$
to indicate that the random variable $Y$ has a binomial distribution
with $N \in \mathbb{N}$ trials, each with a $\theta \in [0, 1]$ chance
of success. The value of the variable $Y$ will be the number of
successes in $N$ trials with a probability $\theta$ of success. The
probability mass function function for $Y$, written $p_Y$, is defined
by
\begin{align}
p_Y(y \mid N, \theta)
&= \textrm{binomial}(y \mid N, \theta)
\\[6pt]
&=
\binom{N}{y} \cdot \theta^y \cdot (1 - \theta)^{N - y}.
\end{align}
Unless necessary for disambiguation, we will drop the random variable
subscripts on probability density or mass functions like $p_Y$ going forward, writing
simply $p(y \mid N, \theta)$ and allowing context to disambiguate.

## A first Stan program

Now lets say we wanted to generate random instantiations of $Y$ for
given values of $N$ and $\theta$. We can do that using the following
Stan program, which we will unpack line by line after its listing.

```{.stan filename="stan/binomial-rng.stan"}
data {
  int<lower=0> N;
  real<lower=0, upper=1> theta;
}
generated quantities {
  int<lower=0, upper=N> y = binomial_rng(N, theta);
}
```

The first thing to notice is that a Stan program is organized into
blocks.  Here we have two blocks, a _data block_ containing declarations
of variables that must be input as data, and a _generated quantities
block_, which not only declares variables, but assigns to them.

The second thing to notice about a Stan program is that the variables
are all declared with types. Stan uses _static typing_, which means
that unlike Python or R, a variable's type is declared in the program
before it is used rather than determined at run time based on what is
assigned to it. Once declared, a variable's type never changes. Stan
also uses _strong typing_, meaning that unlike C or C++, there is no
way to get around the type restrictions and access memory directly.

The program declares three variables, `N` and `y` of type `int`
(integer values in $\mathbb{Z}$), and `theta` of type `real` (real
values in $\mathbb{R}$). On actual computers, our integers will have
fixed upper and lower bounds and our real numbers are subject to all
the vagaries of numerical floating point calculations.

In addition to its basic type, a type may also have constraints.
Because `N` is a count, it must be greater than or equal to zero,
which we indicate with the bound `lower=0`. Similarly, the variable
`y` is the number of successes out of `N` trials, so it must take on a
value between 0 and `N` (inclusive); that is represented with the
constraint `lower=0, upper=N`. Finally, the variable `theta` is real
and declared to fall in the interval $[0, 1]$ with the constraint
`lower=0, upper=1`. Technically, our bounds are open for real values,
but in practice, we might wind up with 0 or 1 values due to underflow
or rounding errors in floating point arithmetic.

At run time, the compiled Stan program must be given values for `N`
and `theta`, at which point, each iteration it will sample a value of
`y` using its built-in pseudorandom number generator. In code, we
first define a dictionary for our data (variables `N` and `theta`),
then construct an instance of `CmdStanModel` for our model from the
path to its program, and finally sample from the model using the
`sample` method of `CmdStanModel`.

```{python}
N = 100
theta = 0.3
data = {'N': N, 'theta': theta}
model = csp.CmdStanModel(stan_file = '../stan/binomial-rng.stan')
sample = model.sample(data = data, seed=123,
                      iter_sampling = 10, iter_warmup = 0, chains = 1,
                      show_progress = False, show_console = False)
```

In the `sample` method of the `CmdStanModel` object, we provide the
data, the pseudorandom number generator seed (for reproducibility of
this case study), the number of sampling iterations (10), the number
of warmup iterations (0, because we are just generating which doesn't
need any warmup), the number of Markov chains (1), and we turn off all
the messages. Our initial setup set the logger level to `WARNING` for
the `cmdstanpy` package in order to get rid of the information-level
messages that would otherwise provide updates on a running Stan program.

The `sample` method returns a sample consisting of the specified
number of draws (10 here). We can extract the draws for the variable
`y` as an array and then print them along with our other variables.

```{python}
y = sample.stan_variable('y')
print("N = ", N, ";  theta = ", theta, ";  y(0:10) =", *y.astype(int))
```

Let's put that in a loop and see what it looks like for 10, 100, 1000,
and 10,000 trials.

```{python}
for N in [10, 100, 1_000, 10_000]:
    data = {'N': N, 'theta': theta}
    sample = model.sample(data = data, seed = 123,
                          iter_sampling = 10, iter_warmup = 0, chains = 1,
			  show_progress = False, show_console = False)
    y = sample.stan_variable('y')
    print("N =", N)
    print("  y: ", *y.astype(int))
    print("  est. theta: ", *(y / N))
```

On the first line for $N = 10$ trials, our simple frequency-based
estimates range from 0.2 to 0.5. By the time we have 10,000 trials,
the frequency-based estimates only vary between 0.292 and 0.309. We
know from the _central limit theorem_ that the spread of estimates is
expected to shrink at a rate of $\mathcal{O}(1 / \sqrt{N})$ for $N$
draws (this result is only asymptotic in $N$, but is very close for
large-ish $N$ in practice).

It is hard to scan these results. Let's take 10,000 trials each time
and plot histograms. The following histogram plots the distribution of
estimates based on 10, 100, and 1000 observations over 100,000 simulated trials.

```{python}
np.random.seed(123)
ts = []
ps = []
theta = 0.3
M = 100_000
for N in [10, 100, 1_000]:
    data = {'N': N, 'theta': theta}
    sample = model.sample(data = data, seed = 123,
                          iter_sampling = M, iter_warmup = 0, chains = 1,
			  show_progress = False, show_console = False)
    y = sample.stan_variable('y')
    theta_hat = y / N
    ps.extend(theta_hat)
    ts.extend(itertools.repeat(N, M))
xlabel = 'estimated Pr[success]'    
df = pd.DataFrame({xlabel: ps, 'trials': ts})
mydraw(
    pn.ggplot(df, pn.aes(x = xlabel))
  + pn.geom_histogram(binwidth=0.01)
  + pn.facet_grid('. ~ trials')
  + pn.scales.scale_x_continuous(limits = [0, 1], breaks = [0, 1/4, 1/2, 3/4, 1],
                                 labels = ["0", "1/4", "1/2", "3/4", "1"],
                                 expand=[0, 0])
  + pn.scales.scale_y_continuous(expand=[0, 0, 0.05, 0])
  + pn.theme(aspect_ratio = 1,
             panel_spacing = 0.1,
             strip_text = pn.element_text(size = 6),
             strip_background = pn.element_rect(height=0.08, fill = "lightgray"),
             axis_text_y = pn.element_blank(),
             axis_ticks_major_y = pn.element_blank(),
             axis_ticks_minor_y = pn.element_blank(),
             axis_title_y = pn.element_blank(),
             axis_text_x = pn.element_text(size = 6),
             axis_title_x = pn.element_text(size = 8))
)            
```

The trial size of 10 only has 10 possible values, 0.0, 0.1, ..., 1.0,
so the histogram (technically a bar chart here) just shows the counts
of those outcomes. Here, $y = 3$ is the most prevalent result, with
corresponding estimate for $\theta$ of $y / 10 = 0.3$. The trial size
of 100 looks roughly normal, as it should as a binomial with trials $N
= 100$. By the time we get to $N = 1,000$ trials, the
draws for $y$ concentrate near 300, or near the estimated value of
$0.3$ for $\theta$. As $N$ grows, the central limit theorem tells us
to expect that the width of these histograms to shrink at a rate of
$\mathcal{O}(1 / \sqrt{N})$.



# Laplace's problem: Male birth ratio

Bayes formulated a mathematical solution to the _inverse problem_ of
determining a posterior distributon with density $p(\theta \mid y)$
from a prior with density $p(\theta)$, a sampling distribution with
density $p(y \mid \theta)$, and observed data $y$.  But he was not able to
solve the integral presented in the denominator of his theorem and
actually determine the form of the posterior.

## Laplace's data on live births

A decade later, @laplace1774 solved the integral that vexed Bayes and
applied Bayes's ideas to the applied problem of estimating whether a
boy is more likely to be born than a girl. Laplace gathered data on
the sexes of babies in live births in Paris between 1745 and 1770.  

sex | live births
---:|:---
female | 105,287
male | 110,312
: Live births in Paris between 1745 and 1770.


## A Stan program for Laplace's problem

Unlike the first Stan model we saw, which only generates data, the
following Stan program is going to allow us to observe the number of
male births ($y$) and the total number of births ($N$) and estimate
the probability of a male birth ($\theta$) as well as the probability
that boys are more likely to be born than girls ($\theta > 0.5$). The
Stan program to do this is as follows.

```{.stan filename="stan/sex-ratio.stan"}
data {
  int<lower = 0> N;  // number of live births
  int<lower = 0, upper = N> y;  // male births
}
parameters {
  real<lower=0, upper=1> theta;  // chance of boy
}
model {
  theta ~ uniform(0, 1);  // uniform prior
  y ~ binomial(N, theta);  // binomial sampling
}
generated quantities {
  int<lower=0, upper=1> boys_gt_girls = theta > 0.5;
}
```

## Parameter and model blocks

In this Stan program, we see that both the number of total births $N$
and the number of male births $y$ are given as data. Then there are
two additional blocks we did not see in our earlier program, a
_parameters block_, which is used to declare unknown values (here the
male birth rate $\theta$), and a _model block_, which is where we
define our target density, typically factored as a prior and sampling
distribution. The parameters block declares the type of `theta`, which
is a real value constrained to fall in the interval $[0, 1]$. The
model block defines the prior, which we take to be uniform over the
possible values for `theta`. The model block also defines the sampling
distribution, which codes the fact that the observed data `y` was generated
from a binomial distribution with `N` trials and `theta` probability
of a male birth. Finally, we have a generated quantities block that
defines a single binary indicator variable, `boys_gt_girls`. This
variable will take the value 1 if the probability of a boy is greater
than the probability of a girl.

## Sampling from the posterior

When we run a Stan program, what Stan returns is a sequence of $M$
random draws, which are approximately identically distributed
according to the posterior,
$$
\theta^{(1)}, \ldots, \theta^{(M)} \sim p(\theta \mid y)
$$
If we were to take $M \rightarrow \infty$, the draws will converge to
being identically drawn from the posterior. With a large enough finite
$M$, the draws will become numerically indistinguishable from true
draws from the posterior.

Stan uses a _Markov chain Monte Carlo_ (MCMC) algorithm, which can
lead to autocorrelation in the random draws from the posterior.  That
is, the draws are not typically independent, with each draw being
correlated (or anti-correlated) with the previous draw.  This
autocorrelation does not introduce bias into the Monte Carlo
estimates.

Stan uses a dynamically adaptive form of Hamiltonian Monte Carlo (HMC)
known as the no-U-turn sampler (NUTS).  NUTS can be hyperefficient in
the sense of generating anticorrelated draws that can lead to more
efficient Monte Carlo estimates than independent draws in some
cases.

We fit Laplace's model by compling the model, constructing a
dictionary for the data, and then calling the `sample` method on the
compiled model with the dictionary. We call the sample method with
1,000 warmup iterations and 10,000 sampling iterations; we are taking
so many draws in order to draw smooth histograms later.

```{python}
model = csp.CmdStanModel(stan_file = '../stan/sex-ratio.stan')
boys = 110312
girls = 105287
data = {'N': boys + girls, 'y': boys}
M = 10_000
sample = model.sample(data = data, seed = 123,
                      iter_sampling = M, iter_warmup = 1000,
		      show_progress = False, show_console = False)
```

As before, we proceed by first extracting the draws for the variables
`theta` and `boys_gt_girls`.

```{python}
theta_draws = sample.stan_variable('theta')
boys_gt_girls_draws = sample.stan_variable('boys_gt_girls')
```

We can plot a histogram of approximate draws $\theta^{(m)} \sim
p(\theta \mid y)$ from the posterior to give us a sense of the
value of $\theta$ and its uncertainty given our observed data $y$.

```{python}
mydraw(
  pn.ggplot(pd.DataFrame({'theta': theta_draws}), pn.aes(x = 'theta')) +
  pn.geom_histogram(color='white') +
  pn.labs(x = 'θ') +
  pn.theme(axis_text_y = pn.element_blank(),
           axis_title_y = pn.element_blank(),  
           axis_ticks_major_y = pn.element_blank())
)
```

All of the draws have a value for $\theta$ between 0.50 and 0.52.  In
the next sections, we will see how to use these draws to estimate a
single value for $\theta$ as well as to compute probabilities, such as
the probability that $\theta > 0.5$ or $\theta > 0.51$.


## Bayesian point estimates

In Bayesian terms, a _point estimate_ for a parameter $\Theta$
conditioned on some observed data $Y = y$ is a single value
$\hat{\theta} \in \mathbb{R}^D$ that in some way summarizes the
posterior $p(\theta \mid y)$.

### Posterior mean estimator

The most common Bayesian point estimate for a parameter is the
posterior mean,
\begin{align}
\widehat{\theta}
&= \mathbb{E}[\theta \mid y]
\\[6pt]
&= \int_{\Theta} \theta \cdot p(\theta \mid y) \, \textrm{d}\theta
\\[6pt]
&= \lim_{M \rightarrow \infty} \, \frac{1}{M} \sum_{m=1}^M \theta^{(m)}
\\[6pt]
&\approx \frac{1}{M} \sum_{m=1}^M \theta^{(m)},
\end{align}
where in the last two lines, each draw is distributed approximately
according to the posterior, $\theta^{(m)} \sim p(\theta \mid y)$.

For Laplace's model, the estimate for the male birth rate $\theta$
conditioned on the birth data $y$ is calculated as the sample mean
for the extracted draws for `theta`.

```{python}
theta_hat = np.mean(theta_draws)
print(f"estimated theta = {theta_hat:.3f}")
```



### Posterior median estimator, quantiles, and intervals

A popular alternative Bayesian point estimate is the _posterior
median_, $\theta^+$.  The median is such that for each dimension
$d \in 1{:}D$,
$$
\Pr[\Theta_d \leq \theta^+_d] = \frac{1}{2}.
$$

The posterior median can be calculated by taking the posterior
median of the draws,
```{python}
theta_plus = np.median(theta_draws)
print(f"estimated (median) theta = {theta_plus:.3f}")
```
Because our posterior distribution is nearly symmetric with Laplace's
data, the posterior median is very close to posterior median.

#### Quantiles

Other posterior quantiles are estimated the same way.  For example,
if we want the posterior 95% quantile, we just take the empirical
95% point in the sorted chain of draws.  For example, here
are the 5% quantile and 95% quantile for Laplace's posterior,
calculated with empirical quantiles.
```{python}
quantile_05 = np.quantile(theta_draws, 0.025)
quantile_95 = np.quantile(theta_draws, 0.975)
print(f"0.05 quantile = {quantile_05:.3f};  0.95 quantile = {quantile_95:.3f}")
```

#### Posterior intervals

Together, the 5% quantile and 95% quantile give us the bounds of
our 90% _central probability interval_.  That is, it's the interval
containing 90% of the posterior probability mass, with half of the
remaining mass (5%) taking on higher values and the other half of the
remaining mass (5%) taking on lower values.
```{python}
print(f"central 90% posterior interval for theta = ({quantile_05:.3f}, {quantile_95:.3f})")
```
Other intervals are computed in the exact same way.

### Posterior mode estimator

A popular non-Bayesian point estimate is the _posterior mode_
$\theta^*$, defined as the point with the highest posterior density,
$$
\theta^* = \textrm{arg max}_\theta \ p(\theta \mid y).
$$
Although the posterior mode can be calculated in Stan using 
optimization rather than sampling, we do not consider it here.
The posterior mode is sometimes called the _maximum a posteriori_
(MAP) estimator.

### Estimation error and bias

The _error_ of an estimate is its difference from the true value,
$$
\textrm{err} = \hat{\theta} - \theta.
$$
Our estimate $\hat{\theta}$ is implicitly a function of the data $y$
and so is $\textrm{err}$, so we can make this explicit and write
$$
\textrm{err}(y) = \hat{\theta}(y) - \theta.
$$

The _bias_ of an estimator is defined as its expected error,
\begin{align}
\textrm{bias}
&= \mathbb{E}[\textrm{err}(Y)]
\\[6pt]
&= \mathbb{E}[\hat{\theta}(Y) - \theta]
\\[6pt]
&= \int_Y \hat{\theta}(y) - \theta \ \textrm{d}y.
\end{align}

The posterior mean is a popular Bayesian estimator for two reasons.
First, it is an _unbiased_ estimator in the sense of having zero
bias.  Second, it has the minimum expected square error among
unbiased estimators, where the _squared error_ of an estimate is
defined by
$$
\textrm{err}^2(y) = (\hat{\theta}(y) - \theta)^2.
$$

The posterior median has the pleasant property of being robust to
outliers. The posterior mode, or maximum a posteriori estimate, is at
least consistent in the sense of converging to the true value as the
data size grows.

We will concentrate on posterior means in this quick introduction to
Bayes and Stan.


### (Markov chain) Monte Carlo error and effective sample size

The Markov chain we use to sample is itself a random variable.
Re-running the sampler will produce slightly different results due to
Monte Carlo error (the error introduced by usingly only a finite
sample of $M$ draws).  

Stan reports Markov chain Monte Carlo _standard error_ along with
its estimates of the mean.  The MCMC standard error is for a scalar
parameter $\theta_d$ is defined to be
$$
\textrm{mcmc-se}
= \frac{\textrm{sd}[\theta_d \mid y]}{N^{\textrm{eff}}},
$$
where the numerator is the standard deviation of the parameter
$\theta_d$ in the posterior and $N^{\textrm{eff}}$ is the _effective
sample size_.  In the usual central limit theorem, the number of
samples appers here.  The effective sample size for a sample of size
$M$ is defined to be
$$
N^{\textrm{eff}}
= \frac{M}{\textrm{IAT}},
$$
where $\textrm{IAT}$ is the _integrated autocorrelation time_.
The IAT can be thought of as the interval between effectively
independent draws in our Markov chain.  If we have low
autocorrelation, $\textrm{IAT}$ will be close to 1 and if the
autocorrelation is higher, it can be much higher.  If the
$\textrm{IAT}$ is much higher than 100, it can become difficult
to estimate.  If the
autocorrelation is negative, the $\textrm{IAT}$ is less than 1 and the
effective sample size is larger than the number of draws.  Thus
$N^{\textrm{eff}}$ is the number of independent draws that would
lead the same error as our correlation draws using a Markov chain.

## Estimating event probabilities

Laplace wasn't looking for a point estimate for $\theta$.  He wanted to
know the probability that $\theta > \frac{1}{2}$ after observing $y$
male births in $N$ trials.  In the notation of probability theory,
he wanted to estimate an event probability.

A subset of parameters is known as an _event_. We can convert
conditions on parameters into events. For example, the condition
$\theta > \frac{1}{2}$ can be turned into the event
$$
A = \left\{ \theta \in \Theta : \theta > \frac{1}{2} \right\}.
$$
Events are what are assigned probabilities by a _measure_ in probability
theory. Given a probability measure, the probability of the event $A$,
that the rate of boy births is higher than girl births, will be well defined.
Because we can convert conditions to events, we will be
sloppy and treat the conditions as if they were events. This allows us
to write $\Pr[\Theta > \frac{1}{2} \mid N, y]$ for the probabilty of
the event $\Theta > \frac{1}{2}$.

Technically, we will need to use the _indicator function_ $\textrm{I}$,
which maps propositions like $\theta > \frac{1}{2}$ into the value 1
if they are true and 0 if they are false.  

### Event probabilities via indicators

Event probabilities are defined as posterior conditional expectations
of indicator functions for events.
\begin{align}
\Pr[\Theta > 0.5 \mid N, y]
&= \mathbb{E}\!\left[\textrm{I}[\Theta > 0.5] \mid N, y\right]
\\[8pt]
&= \int_{\Theta} \textrm{I}(\theta > 0.5) \cdot p(\theta \mid N, y) \, \textrm{d}\theta
\\[8pt]
&\approx \frac{1}{M} \sum_{m=1}^M \textrm{I}(\theta^{(m)} > 0.5),
\end{align}
where we assume $\theta^{(m)} \sim p(\theta \mid N, y)$ is distributed
according to the posterior for $m \in 1{:}M$. Following physics
conventions, we use square brackets for functors (functions that apply
to functions); that means we write $\textrm{I}[\cdot]$ when we apply
the indicator function to a random variable and 
$\textrm{I}(\cdot)$ when we apply it to a primitive scalar.

### Events as indicators in Stan

In Stan, we code the value of the indicator function directly
and assign it to a variable in the generated quantities block.
```stan
generated quantities {
  int<lower=0, upper=1> boys_gt_girls = theta > 0.5;
}  
```
Conditional expressions like `theta > 0.5` take on the value
1 if they are true and 0 if they are false.  As is conventional in
languages such as C++, the indicator function itself isn't written.

### The answer to Laplace's question

The posterior mean of the variable `boys_gt_girls` is thus our estimate
for $\Pr[\theta > 0.5 \mid N, y]$.  It is essentially 1.  Printing to
15 decimal places, we see

```{python}
Pr_boy_gt_girl = np.mean(boys_gt_girls_draws)
print(f"estimated Pr[boy more likely] = {Pr_boy_gt_girl:.15f}")
```

The value of 1 returned as an estimate brings up the important
problem of numerical precision.  As we can see from the histogram,
all of our sampled values for $\theta$ are greater than $\frac{1}{2}$.

Laplace calculated the result analytically, which is
$$
\Pr\!\left[\theta > \frac{1}{2}\right] \approx 1 - 10^{-27}.
$$
Thus we would need an astronomical number of posterior draws
before we would generate a value of $\theta$ less than $\frac{1}{2}$.
As given, the answer of 1.0 is very close to the true answer and well
within our expected Monte Carlo error.


### MCMC summary statistics from Stan

With Stan, we can print a summary for the variable $\theta$ in the
posterior, which reports all of these values.  We just call The
`.summary()` function on the sample.  

```{python}
sample.summary(sig_figs=3)
```

The rows are for our random variables, with `lp__` indicating the
unnormalized log density defined by the Stan program (which includes
change of variable adjustments for constrained parameters).  The
other two rows are for variables defined in the Stan program, `theta`,
and `boys_gt_girls`.  The number of significant figures used in the
results can be controlled in the summary function, as can the
quantiles being reported.

The first column reports the posterior mean, and agrees with our
earlier calculations for both variables.  The second column is
the Monte Carlo standard error (based on an estimated effective
sample size) and a posterior standard deviation estimate.  The
next three columns are quantiles we computed earlier, and they also
agree with our calculations.  Next is the effective sample size,
which can vary from variable to variable, and the effective sample
size rate (per second).  The final column reports $\widehat{R}$, which we
discuss in the next section.

# Warmup and convergence monitoring

When runnig Markov chains, we want to make sure that we have moved far
enough that our draws are approximately from the posterior.  A
standard way to monitor convergence is to start multiple Markov chains
at different initializations (ideally chosen from a diffuse
initialization like a draw from the prior) and measure whether they
are producing draws from the same distirbution.

## Warmup

Stan performs a number of _warmup iterations_ during which time it
tries to find the region in which it should be sampling and then adapt
a good step size and estimate the posterior covariance (just the
diagonal with the variances by defaul).  The step size is used in the
Hamiltonian dynamics simulator and the (co)variance is used as a
preconditioner.

Warmup converges when the step size and posterior covariance estimates
no longer change.  With multiple chains, it's possible to test that
they have all converged to the same step size and covariance estimate.
Typically, we don't bother doing this and just measure our end goal
directly, which is whether we are getting reasonable posterior draws
after warmup.

Warmup doesn't form a single coherent Markov chain because it uses
memory to adapt.  Once Stan starts sampling, the result is a Markov
chain.  All of our posterior analysis will be with draws from the
Markov chain, not from warmup.  We can save and extract the warmup
draws to investigate the behavior of warmup.


## Potential scale reduction and $\widehat{R}$

Stan uses the _potential scale reduction_ statistic $\widehat{R}$
(pronounced ``R hat'').  Given a sequence of Markov chains, Stan
splits them in half to make sure the first halfs and second halfs of
the chain agree, then calculates variances within each chain and
across all chains and compares.  The statistic $\widehat{R}$ converges
to 1 as the Markov chains converge to the same distributions.

## Practical guidelines

A simple rule of thumb is to run chains as long as necessary with equal
numbers of warmup and sampling iterations until the sampling
iterations have $\hat{R} < 1.01$.  Running more warmup iterations is
important because sampling will not be efficient if warmup has not
converged and it at most uses a few more iterations.

We also want to make sure that our effective sample size is at least
25 per chain.  It is not that we need so many draws for inference, but
that we do not trust our effective sample size estimator if it is much
lower.   One way to check that the ESS estimator is OK is to double
the number of draws and make sure that the ESS also doubles.  If it
doesn't, it's a sign that the first ESS estimate is unreliable.

## Multi-threading

You can run chains in parallel, but you probably don't want to run
more chains than you have physical cores on your CPU (hardware tends
to report the ``virtual'' cores as well as physical cores).  But you
may want to run even fewer if your chains are memory bound.  You can
test the most efficient number of parallel chains to use for your
hardware, operating system, and C++ compiler and settings.



# Appendices

# Expectation notation

If we have a (potentially multivariate) random variable $Z$, we write
$\mathbb{E}[Z]$ for its _expectation_, which is defined as its average
value.  This assumes we have a fixed measure to provide a density
$p_Z(z)$ for the random variable $Z$.  We compute the average value by
averaging over the possible values for $Z$ with weights given by its
density,

$$
\mathbb{E}[Z] = \int_{Z} z \cdot p_Z(z) \, \textrm{d}z.
$$

With Bayesian statistics, we are typically interested in conditional
expectations, which is the value of a random variable conditioned on
the observed value of a second random variable.  Suppose $Y$ is a
second random variable and we observe that $Y = y$.  The _conditional
expectation_ of $Z$ given $Y = y$ is written
$$
\mathbb{E}[Z \mid Y = y]
= \int_{Z} z \cdot p(z \mid y) \, \textrm{d}z.
$$
That is, we take a weighted average of the value of $Z$ with weights
determined by the posterior density $p_{Z \mid Y}(z \mid y)$.  As
before, this assumes a fixed, joint measure for $Y$ and $Z$.

We are typically concerned with cases where $Z = f(\Theta)$ is a
function of the parameters $\Theta$ (and perhaps some data, as well),
and the conditioning is with respect to observed data $y$, where we
will drop the explicit $Y = y$ conditioning and just use $y$,
$$
\mathbb{E}[f(\Theta) \mid y]
= \int_{\Theta} f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$
These posterior expectations simply average the value of the function
$f(\theta)$ over the posterior $p(\theta \mid y)$.  All of our
inferences proceed this way---by averaging a quantity of interest
expressed as a function of parameters over the posterior



# Quantiles, medians, and uncertainty intervals

Suppose $Z$ is a univariate random variable (e.g., one of the
parameters, $\Theta_d$).  If $p \in [0, 1]$, the _$p$-quantile_ for a
univariate random variable $Z$, $\textrm{quantile}(Z, p)$, is defined
by
$$
\textrm{quantile}(Z, p) = z
\ \textrm{ if and only if } \
\textrm{Pr}[Z \leq z] = p.
$$
That is, the $p$-quantile of $Z$ is the value $z$ such that the
probability $Z$ is less than or equal to $z$ is $p$.  In Bayesian
applications, we will typically be taking the probability for
functions of parameters $Z = f(\Theta)$ and conditioning on observed data $y$.

The 0.5-quantile is known as the _median_.  There is a 50% chance that
a random variable takes on a value less than its median and a 50%
chance that it takes on a value greater.  Another common Bayesian
estimator for parameters $\Theta$ is their posterior median,
$$
\overline{\theta} = \textrm{quantile}(Z, 0.5).
$$
The median estimator has the property of minimizing expected absolute
error (in contrast to the posteiror mean estimate, which minimizes
expected squared error), again assuming the model represents the true
data generating process.


If we have two probabilities, $0 \leq p^L \leq p^U \leq 1$, they
define the $(p^L, p^U)$ _posterior interval_ 
$$
\textrm{interval}(Z, p^L, p^U)
= \left(
    \textrm{quantile}(Z, p^L),
    \textrm{quantile}(Z, p^U)
  \right).
$$

The probability that a point falls in the interval is given by the
differences in probabilities,
$$
\textrm{Pr}\!\left[Z \in \textrm{interval}(Z, p^L, p^U)\right] = p^U - p^L.
$$
If $1 - p^U = p^L$, the interval is called the _central $(p^L - p^U)$ interval_.
For example, the central 95% interval for $Z$ is
$\left( \textrm{quantile}(Z, 0.025), \textrm{quantile}(Z, 0.975) \right)$.





# Bayesian statistics

@bayes1763 introduced the paradigm of statistical inference that has
come to be known as Bayesian statistics.  In retrospect, Bayes's idea
is quite simple, involving four conceptually simple steps.

1.  Define a parametric _sampling density_ $p(y \mid \theta)$ which
describes how to generate observations $y$ given parameters $\theta$.
2.  Define a _prior density_ $p(\theta)$ capturing what is known before
observing new data.
3.  Derive the _posterior density_ $p(\theta \mid y) \propto p(y \mid
\theta) \cdot p(\theta)$ via Bayes's rule.
4.  Evaluate _event probabilities_ conditioned on observed data $y$,
such as $\textrm{Pr}[\textrm{cond}(\theta) \mid y]$ for some condition
on the parameters by integrating over the posterior. 

We work through an example end-to-end in the next section.

## Bayes's theorem

Bayes
formalized his approach in the following theorem.

::: {#thm-line}
### Bayes's Theorem

Given a joint density $p(y, \theta)$, the posterior density $p(\theta
\mid y)$ can be defined in terms that only involve the prior
$p(\theta)$ and sampling distribution $p(y \mid \theta)$, as
$$
p(\theta \mid y)
\ = \
\frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y \mid \theta) \cdot p(\theta) \,
        \textrm{d}\theta}.
$$

_Proof_:
\begin{align}
p(\theta \mid y)
&=  \frac{p(y, \theta)}
         {p(y)}
& \textrm{[definition of conditional probability]}       
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {p(y)}
& \textrm{[chain rule]} 
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y, \theta) \, \textrm{d}\theta}
& \textrm{[law of total probability]}
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y \mid \theta) \cdot p(\theta) \,
        \textrm{d}\theta}.
& \textrm{[chain rule]} 
\end{align}
$\blacksquare$
:::

Bayes's theorem allows us to solve the so-called _inverse problem_ of
inferring the posterior $p(\theta \mid y)$ when all we have is the
sampling distribution $p(y \mid \theta)$, the prior $p(\theta)$, and
some observed data $y$.

In most casses, Stan programs only require densities defined up to a
normalizing constant.   This allows us to go a step further and drop
the denominator $p(y)$, which does not depend on $\theta$.
$$
p(\theta \mid y)
\ \propto \
p(y \mid \theta) \cdot p(\theta).
$$

This lets us proceed with only an unnormalized sampling density $p(y
\mid \theta)$ and unnormalized prior $p(\theta)$.  For some
applications to model comparison we will need a normalizing sampling
distribution $p(y \mid \theta)$ in order to make and compare
probabilistic predictions.

# Bayesian inference

Bayesian inference is largely about estimating quantities of interest
based on a probability model and observed data.  In typical applied
Bayesian inference problems, we are interested in three quantities
that can be expressed as expectations: parameter estimates, event
probabilities, and probabilistic prediction.  All of these quantities
are expressed as expectations over the posterior, meaning that they
involve averaging over our uncertainty in parameter values.

We are also interested in uncertainty, which is defined by the
posterior.  We typically summarize uncertainty using quantiles, and in
particular, posterior intervals (sometimes called "credible
intervals").


## Parameter estimation

The first quantity of interest is the value of parameters.  The
standard Bayesian parameter estimate is the posterior mean, or
conditional expectation given the data.  Given a model $p(y, \theta)$
and observed data $y$, the Bayesian posterior mean estimate of the
parameters $\theta$ is

\begin{align}
\widehat{\theta}
&= \mathbb{E}[\Theta \mid y]
\\[6pt]
&= \int_{\Theta} \theta \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

The posterior mean as a parameter estimate minimizes expected square
error in the estimates if the model is well specified in the sense of
representing the true data-generating process.  Squared error is the squared
$\textrm{L}_2$ norm of the difference between the estimate and the true
parameter values.  We can expand these definitions down to basic form.

\begin{align}
\widehat{\theta}
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. \left|\left| \, u - \Theta \, \right|\right|^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. (u - \Theta)^{\top} \cdot (u - \Theta)  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. \sum_{d=1}^D \, (u_d - \Theta_d)^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \sum_{d=1}^D \, \mathbb{E}\!\left[ \left. (u_d - \Theta_d)^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \sum_{d=1}^D \, \int_{\Theta} (u_d - \theta_d)^2 \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

It's clear from the final form that the estimate $\widehat{\theta}$ is determined
by averaging over posterior uncertainty. 


## Event probabilities

An _event_ in statistics is a subset of the parameter space, $A
\subseteq \Theta$, where $\Theta$ is the set of all valid parameter
values.  We usually pick out events using conditions on the parameter
space.  For example, the condition $\theta > 0.5$ defines the
event $A = \left\{ \theta \in \Theta : \theta > 0.5 \right\}$.

The probability of an event conditioned on data is just another
posterior expectation, this time of an indicator variable.

\begin{align}
\textrm{Pr}[A \mid y]
&= \mathbb{E}[\textrm{I}[\Theta \in A] \mid y]
\\[6pt]
&= \int_{\Theta} \textrm{I}(\theta \in A) \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

The expression $\textrm{I}(\theta \in A)$ takes on value 1 if $\theta
\in A$ and value 0 otherwise.  We write $\textrm{I}[\Theta \in A]$
using square brackets because a random variable is a function, whereas
we write $\textrm{I}(\theta \in A)$ because $\theta \in \mathbb{R}^D$
is a value.


## Posterior predictive inference

Often we are interested in predicting new data $\tilde{y}$ given the
observation of existing data $y$.  This is a form of _posterior
predictive inference_.  For example, $y$ might be the price of a stock
over some time period and $\tilde{y}$ the price of the stock in the
future.  Or $y$ might be the result of past games and $\tilde{y}$ the
winner of tomorrow's game.  Posterior predictive inference is also
cast an expectation, this time of a density.

\begin{align}
p(\tilde{y} \mid y)
&= \mathbb{E}\!\left[ \, p(\tilde{y} \mid \Theta) \mid y \, \right]
\\[6pt]
&= \int_{\Theta} p(\tilde{y} \mid \theta) \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}



# Markov chain Monte Carlo

Stan performs asymptotically exact, full Bayesian inference using
Markov chain Monte Carlo (MCMC).  MCMC starts from an initial value
$\theta^{(0)}$, which can be random, user provided, or a mixture of
both, and then generates a sequence of values
$\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(m)}, \ldots$ forming a _Markov chain_.  
A sequence forms a Markov chain if each value is generated conditioned on
only hte previous value, so that

$$
\theta^{(m + 1)} \sim q(\theta \mid \theta^{(m)}.
$$

Because of this dependency, the sequence of random variables making up
the Markov chain exhibit a degree of _autocorrelation_, meaning that
draw $\theta^{(m)}$ is correlated with draw $\theta^{(m+1)}$.

We typically assume some mild conditions on the Markov chain
transition.  First, it must be _ergodic_ in the sense of never getting
stuck in such a way it can't visit the rest of the parameter space.
Second, it must be _aperiodic_ in the sense of not cycling regularly
through regions of the parameter space.  Third, it needs to preserve
the target density in the sense that if  $\theta^{(m)} \sim
p(\theta \mid y)$ has the posterior as its marginal distribution, then
$\theta^{(m+1)} \sim p(\theta \mid y)$ also follows the posterior
distribution.  

If the initial value $\theta^{(0)}$ is drawn from the posterior,
$$
\theta^{(0)} \sim p(\theta \mid y),
$$
then each value in the Markov chain will be marginally distributed
according to the posterior,
$$
\theta^{(1)}, \ldots, \theta^{(M)}
\sim p(\theta \mid y).
$$

Typically we cannot draw an initialization from the posterior---if we
could, we'd just use simpler Monte Carlo methods without the Markov
chain dependencies.  In the situation where $\theta^{(0)}$ is not a
draw from the posterior $p(\theta \mid y)$, then as the chain
progresses, it will converge to the right distribution in the sense
that in the limit as $m \rightarrow \infty$, the draws approach the
corect distribution $\theta^{(m)} \sim p(\theta \mid y)$. 

Now suppose we have a psoterior sample of draws $\theta^{(1)}, \ldots,
\theta^{(M)}$. We can estimate expectations by simply plugging in
values of $\theta^{(m)}$ and averaging.  If our chain satisfies the
mild conditions above, we know that we get the right answer asymptotically,

\begin{align}
\mathbb{E}\!\left[f(\theta) \mid y \right]
&= \int_{\Theta} f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta
\\[6pt]
&= \lim_{M \rightarrow \infty} \ \frac{1}{M} \sum_{m=1}^M f\!\left( \theta^{(m)} \right).
\end{align}.

With only finite time in practice, we use the initial segment of the
chain to estimate expectations,
$$
\mathbb{E}\!\left[f(\theta) \mid y\right]
\approx \frac{1}{M} \sum_{m=1}^M f(\theta^{(m)}).
$$

If our initial draw is from the posterior, this estimate is unbiased.
In the usual situation, where the initial draw is not from the
posterior, our expectation retains a degree of bias. Nevertheless, the
limit above shows that the bias goes to zero in the limit. In
practice, we typically remove an initial segment of $N$ draws before
the chain has approximately converged to the right distribution, and
average the remaining draws,
$$
\mathbb{E}\!\left[f(\theta) \mid y\right]
\approx \frac{1}{M - N} \sum_{m=N + 1}^M f(\theta^{(m)}).
$$

Given an estimate $\widehat{\theta}$, its _error_ is $\widehat{\theta}
- \theta$. If its expected error is zero, an estimator is said to be
_unbiased_.   If the Monte Carlo draws $\theta^{(1)}, \ldots, \theta^{(M)}$ are
independent, the _central limit theorem_ tells us that our error
follows a normal distribution asymptotically, so that as $M \rightarrow \infty$,
the error of our estimate $\widehat{\theta}$ follows a normal distribution,
$$
\widehat{\theta} - \theta
\sim \textrm{normal}\!\left(0, \frac{\sigma}{\sqrt{M}}\right),
$$
where $\sigma$ is the standard deviation of the variable $\theta$.
Pre-asymptotically, this result holds approximately and will be used
to estimate the distribution of estimation errors.

Because draws in MCMC can be correlated (or even anticorrelated), we
need the MCMC CLT to genralize the central limit theorem to the case
of correlated variables. Here, we can estimate an _effective sample
size_ (ESS), which is the number of independent draws that provide the same
error distribution.  If $M^{\textrm{eff}}$ is the effective sample size of the
Markov chain $\theta^{(1)}, \ldots, \theta^{(M)}$, then the error will
be distributed approximately as

$$
\widehat{\theta} - \theta
\sim \textrm{normal}\!\left(0, \frac{\sigma}{\sqrt{M^{\textrm{eff}}}}\right).
$$

# References