---
title: "Getting Started with Stan"
subtitle: "in Python with `cmdstanpy` and `plotnine`"
author: "Bob Carpenter"
date: "March 27, 2023"
theme:
  - cosmo
  - custom.scss
format:
  html:
    highlight-style: arrow
    mainfont: Palatino
    monofont: "Menlo, Lucida Console"
    fontsize: 14pt
    linestretch: 1.5
    number-sections: true
    number-depth: 2
    toc: true
    toc-location: right
    code-fold: true
    code-copy: true
    css: style.css
  pdf:
    include-in-header:
      - file: header.tex
    mainfont: Palatino
#    monofont: SourceCodePro
    monofontoptions:
      - Scale=0.6
    number-sections: true
    number-depth: 2
#    shift-heading-level-by: -1
    margin-bottom: 1in
    fig-pos: "t!"
    biblio-title: "References"
    biblio-style: natbib
    link-citations: true
    link-bibliography: true
    pdf-engine: xelatex
    highlight-style: github
bibliography: references.bib
---

# Introduction

These notes are intended to introduce several technical topics to
practitioners: Bayesian statistics and probabilistic modeling, Markov
chain Monte Carlo methods for Bayesian inference, and the Stan
probabilistic programming language.


## Bayesian statistics

The general problem addressed by statistical inference is that of
reasoning from a limited number of noisy observations. For example, we
might want to perform inference about a population after measuring a
subset of its members, or we might want to predict future events after
observing past events.

There are many ways to go about applied statistics.  These notes focus
on Bayesian statistics, a form of statistical modeling and inference
that is grounded in probability theory.  In the Bayesian
approach to statistics, we characterize our knowledge of the world in
terms of probabilities (e.g., there is a 24.3% chance of rain after
lunch today, the probability that the next baby born in the United
states is male is 51\%).

Bayesian inference is always carried out with respect to a
mathematical model of a stochastic data generating process. If the
model is well-specified in the sense of matching the true data
generating process, then Bayesian statistical inference can be shown
to have several desirable properties, such as calibration and
resistance to overfitting.

As an Appendix, I have provided a short introduction to Bayesian
inference, including Markov chain Monte Carlo as used by Stan.  This
establishes both the notation and what we are trying to compute with
Stan.

## Markov chain Monte Carlo methods

Bayesian inference for parameter estimation, prediction, or event
probability estimation is based on posterior expectations.  A
posterior expectation is a high dimensional integral.  The standard
approach to solving high-dimensional integrals is the Monte Carlo
method.  For statistical models, where the expectation is over an
unnormalized density from which we cannot easily take random samples,
we have to resort to Markov chain Monte Carlo (MCMC) methods.

## Stan

Stan is an expressive probabilistic programming language that allows a
wide range of statistical models to be coded.  At its heart, a
Stan program declares data variables, parameters, and defines a
posterior log density up to a constant (typically by defining the
joint log density over parameters and data).

Stan programs are translated to C++ and fit with either MCMC or
approximate methods.  Users provide Stan data and access Stan output
through analytics languages like Python, R, or Julia.  This note
focuses on MCMC-based inference and Python.
  
## Pre-requisites

I will assume the reader is familiar with the basic notions of
differential and integral calculus in multiple dimensions (e.g., the
typical first- and second-year undergraduate sequence).  But don't
worry, we only need calculus to define what we are computing---the
actual calculations will all be done by Stan.  I will further assume
that the reader is familiar with the basics of matrix operations
(e.g., as taught in an intro to linear algebra class).  

I will further assume that the reader is familiar with basic notions
of probability theory, including discrete and continuous densities,
probability density and mass functions, cumulative distribution
functions, and the basic rules of probability theory (e.g., as taught
in an introduction to mathematical statistics).

I will also assume the reader is familiar with basic Python numerical
programming, including NumPy and SciPy (there are a lot of tutorials
available online and in book form).  I'll be using [Python
3](https://www.python.org/downloads/) with the
[CmdStanPy](https://mc-stan.org/cmdstanpy/installation.html) interface
to Stan.  Click the links and follow the install instructions if you
would like to follow along in code.

## Which Stan interface should I use?

Stan can be accessed directly from the command line or through
analysis languages like Python, R, or Julia.  The compiled model class
can also be accessed directly in these languages.

### CmdStan

The reference implementation of Stan is CmdStan, which is
a command-line interface (CLI) written entirely in C++.  

* [CmdStan](https://mc-stan.org/users/interfaces/cmdstan): CLI

CmdStan produces comma-separated value (CSV) output, which makes it
easy to read into languages such as R, Python, or Julia for further
analysis.

### Out of process interfaces

I would strongly recommend one of the following interfaces, which
write data to a file, execute a separate process for CmdStan, then
read the results in through the file system.

* [CmdStanPy](https://mc-stan.org/cmdstanpy/): Python
* [CmdStanR](https://https://mc-stan.org/cmdstanr/): R
* [Stan.jl](https://mc-stan.org/users/interfaces/julia-stan): Julia

The out-of-process architecture makes these interfaces relatively easy
to install and keep up to date with Stan because there are no C++
compatibility issues to work out between the analysis languages and
Stan.  This case study uses CmdStanPy.

### In process interfaces

The following interfaces were the first two developed for Stan.

* [RStan](http://mc-stan.org/rstan/): R
* [PyStan](https://pystan.readthedocs.io/en/latest/index.html): Python

The Python in-memory interface was later replaced with the following
HTTP-based web service.

* [HttpStan](https://github.com/stan-dev/httpstan): HTTP

Both RStan and HttpStan (and hence PyStan) rely on high-level foreign
function interfaces (FFI) to Stan (specifically Cython and Rcpp),
which means they invoke Stan in memory in the same process running R
or HttpStan.  This requires compatibility between the C++ compiler
used for Python and R and the the one used for compiling a Stan program.
This in turn complicates installation, particularly on Windows.

### Model interface

We have recently developed a more portable in-memory interface to
Stan's log density function along with gradients and Hessians, and the
constraining and unconstraining parameter transforms.  These are coded
using a relatively portable low-level memory interface (`.C` in R,
`.ctypes` in Python).

* [BridgeStan](https://github.com/roualdes/bridgestan): R, Python, Julia, C, Rust 



## Python boilerplate

We include the following Python boilerplate to import and configure
packages we will use throughout these notes.

```{python}
# PROJECT SETUP
import cmdstanpy as csp

import numpy as np

# stop plotnine griping
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings( "ignore", module = "plotnine\..*" )

# stop stan informational updates and warnings
import logging
csp.utils.get_logger().setLevel(logging.ERROR)

import pandas as pd
import plotnine as pn
import itertools

def mydraw(x):
    """draw plot in the quarto doc"""
    x.draw()
```

# Stan for forward simulation

We're first going to consider simple binomial sampling in order to
introduce Stan programs and how they're called and to develop some
intuitions about estimation based on binary outcomes.

Let's say we have 100 trials, each with a 30% chance of success. The
30% might represent a chance of rain and the result the number of days
out of 100 that it actually rained; it might represent the chance of a
drug improving a patient's condition with the result being the number
of patients who improve; or, it might represent the chance of a
successful penalty kick from a given position on a field, and the the
total number representing the number of goals in 100 attempts.

In statistical sampling notation, we write
$$
Y \sim \textrm{binomial}(N, \theta)
$$
to indicate that the random variable $Y$ has a binomial distribution
with $N \in \mathbb{N}$ trials, each with probability $\theta \in [0, 1]$
of success. The value of the variable $Y$ will be the number of
successes in $N$ independent trials with a probability $\theta$ of success. The
probability mass function function for $Y$, written $p_Y$, is defined
for $y \in 0{:}N$ by
by
\begin{align}
p_Y(y \mid N, \theta)
&= \textrm{binomial}(y \mid N, \theta)
\\[6pt]
&=
\binom{N}{y} \cdot \theta^y \cdot (1 - \theta)^{N - y}.
\end{align}
Unless necessary for disambiguation, we will drop the random variable
subscripts on probability density or mass functions like $p_Y$ going forward, writing
simply $p(y \mid N, \theta)$ and allowing context to disambiguate.

## A first Stan program

Now let's say we wanted to generate random instantiations of $Y$ for
given values of $N$ and $\theta$. We can do that using the following
Stan program, which we will unpack line by line after its listing.

```{.stan filename="stan/binomial-rng.stan"}
data {
  int<lower=0> N;
  real<lower=0, upper=1> theta;
}
generated quantities {
  int<lower=0, upper=N> y = binomial_rng(N, theta);
}
```

The first thing to notice is that a Stan program is organized into
blocks.  Here we have two blocks, a _data block_ containing declarations
of variables that must be input as data, and a _generated quantities
block_, which not only declares variables, but assigns to them.

The second thing to notice about a Stan program is that the variables
are all declared with types. Stan uses _static typing_, which means
that unlike Python or R, a variable's type is declared in the program
before it is used rather than determined at run time based on what is
assigned to it. Once declared, a variable's type never changes. Stan
also uses _strong typing_, meaning that unlike C or C++, there is no
way to get around the type restrictions and access memory directly.

The program declares three variables, `N` and `y` of type `int`
(integer values in $\mathbb{Z}$), and `theta` of type `real` (real
values in $\mathbb{R}$). On actual computers, our integers will have
fixed upper and lower bounds and our real numbers are subject to all
the vagaries of numerical floating point calculations.

In addition to its basic type, a type may also have constraints.
Because `N` is a count, it must be greater than or equal to zero,
which we indicate with the bound `lower=0`. Similarly, the variable
`y` is the number of successes out of `N` trials, so it must take on a
value between 0 and `N` (inclusive); that is represented with the
constraint `lower=0, upper=N`. Finally, the variable `theta` is real
and declared to fall in the interval $[0, 1]$ with the constraint
`lower=0, upper=1`. Technically, our bounds are open for real values,
but in practice, we might wind up with 0 or 1 values due to underflow
or rounding errors in floating point arithmetic.

At run time, the compiled Stan program must be given values for `N`
and `theta`, at which point, each iteration it will sample a value of
`y` using its built-in pseudorandom number generator. In code, we
first define a dictionary for our data (variables `N` and `theta`),
then construct an instance of `CmdStanModel` for our model from the
path to its program, and finally sample from the model using the
`sample` method of `CmdStanModel`.

```{python}
N = 100
theta = 0.3
data = {'N': N, 'theta': theta}
model = csp.CmdStanModel(stan_file = '../stan/binomial-rng.stan')
sample = model.sample(data = data, seed = 123, chains = 1,
                      iter_sampling = 10, iter_warmup = 0,
                      show_progress = False, show_console = False)
```

In the `sample` method of the `CmdStanModel` object, we provide the
data, the pseudorandom number generator seed (for reproducibility of
this case study), the number of sampling iterations (10), the number
of warmup iterations (0, because we are just generating which doesn't
need any warmup), the number of Markov chains (1), and we turn off all
the messages. Our initial setup set the logger level to `WARNING` for
the `cmdstanpy` package in order to get rid of the information-level
messages that would otherwise provide updates on a running Stan program.

The `sample` method returns a sample consisting of the specified
number of draws (10 here). We can extract the draws for the variable
`y` as an array and then print them along with our other variables.

```{python}
y = sample.stan_variable('y')
print("N = ", N, ";  theta = ", theta,
      ";  y(0:10) =", *y.astype(int))
```

Let's put that in a loop and see what it looks like for 10, 100, 1000,
and 10,000 trials.

```{python}
for N in [10, 100, 1_000, 10_000]:
    data = {'N': N, 'theta': theta}
    sample = model.sample(data = data, seed = 123, chains = 1,
                          iter_sampling = 10, iter_warmup = 0,
                          show_progress = False,
			  show_console = False)
    y = sample.stan_variable('y')
    print("N =", N)
    print("  y: ", *y.astype(int))
    print("  est. theta: ", *(y / N))
```

On the first line for $N = 10$ trials, our simple frequency-based
estimates range from 0.2 to 0.5. By the time we have 10,000 trials,
the frequency-based estimates only vary between 0.292 and 0.309. We
know from the _central limit theorem_ that the spread of estimates is
expected to shrink at a rate of $\mathcal{O}(1 / \sqrt{N})$ for $N$
draws (this result is only asymptotic in $N$, but is very close for
large-ish $N$ in practice).

It is hard to scan these results. Let's take 10,000 trials each time
and plot histograms. The following histogram plots the distribution of
estimates based on 10, 100, and 1000 observations over 100,000 simulated trials.

```{python}
np.random.seed(123)
ts = []
ps = []
theta = 0.3
M = 100_000
for N in [10, 100, 1_000]:
    data = {'N': N, 'theta': theta}
    sample = model.sample(data = data, seed = 123, chains = 1,
                          iter_sampling = M, iter_warmup = 0,
                          show_progress = False,
			  show_console = False)
    y = sample.stan_variable('y')
    theta_hat = y / N
    ps.extend(theta_hat)
    ts.extend(itertools.repeat(N, M))
xlabel = 'estimated Pr[success]'    
df = pd.DataFrame({xlabel: ps, 'trials': ts})
mydraw(
    pn.ggplot(df, pn.aes(x = xlabel))
  + pn.geom_histogram(binwidth=0.01)
  + pn.facet_grid('. ~ trials')
  + pn.scales.scale_x_continuous(limits = [0, 1],
                         breaks = [0, 1/4, 1/2, 3/4, 1],
                         labels = ["0", "1/4", "1/2", "3/4", "1"],
                         expand=[0, 0])
  + pn.scales.scale_y_continuous(expand=[0, 0, 0.05, 0])
  + pn.theme(aspect_ratio = 1,
             panel_spacing = 0.1,
             strip_text = pn.element_text(size = 6),
             strip_background = pn.element_rect(height=0.08,
	                            fill = "lightgray"),
             axis_text_y = pn.element_blank(),
             axis_ticks_major_y = pn.element_blank(),
             axis_ticks_minor_y = pn.element_blank(),
             axis_title_y = pn.element_blank(),
             axis_text_x = pn.element_text(size = 6),
             axis_title_x = pn.element_text(size = 8))
)            
```

The trial size of 10 only has 10 possible values, 0.0, 0.1, ..., 1.0,
so the histogram (technically a bar chart here) just shows the counts
of those outcomes. Here, $y = 3$ is the most prevalent result, with
corresponding estimate for $\theta$ of $y / 10 = 0.3$. The trial size
of 100 looks roughly normal, as it should as a binomial with trials $N
= 100$. By the time we get to $N = 1,000$ trials, the
draws for $y$ concentrate near 300, or near the value of
$0.3$ for $\theta$. As $N$ grows, the central limit theorem tells us
to expect that the width of these histograms to shrink at a rate of
$\mathcal{O}(1 / \sqrt{N})$.


## Monte Carlo and Markov chain Monte Carlo methods

In the previous section, we generated a sample of draws by taking each
value to be an independent coin flip.  The estimates for the true
probability of heads based on the sample frequency of positive outcomes
concentrated around the true value as the number of draws increased.

For our applied Bayesian modeling problems, we will not be able to
take independent draws from the densities of interest.  Instead, we
will only be able to construct a _Markov chain_ in which each element
is marginally drawn from the density of interest, but may come with
correlation or anti-correlation.  Markov chains are sequences of
random variables, each of which is conditionally independent given
only the previous element.  That is, a sequence of random variables
$Y_1, Y_2, \ldots$ makes up a Markov chain if
$$
p_{Y_{n+1} | Y_{1}, \ldots Y_N}(y_{n + 1} | y_1, \ldots, y_n)
=
p_{Y_{n+1} \mid Y_n}(y_{n+1} \mid y_n)
$$

We can illustrate with a simple example of three Markov chains, all of
which have a stationary distribution of
$\textrm{bernoulli}(0.5)$ and thus the long-term average of all chains
should approach 0.5.  We will introduce a parameter $\theta \in (0, 1)$ and
take the probabilities of element $Y_{n+1}$ depending on the previous
element $Y_{n}$ to be
\begin{align}
\Pr[Y_{n + 1} &= 1 \mid Y_n = 1] = \theta
\\
\Pr[Y_{n + 1} &= 1 \mid Y_n = 0] = 1 - \theta
\end{align}
The first line says that if the last number we generated is 1, the
probability of the next element being 1 is $\theta$.  The second line
says that if the last number we generated is 0, the probability of the
next element being 1 is $1 - \theta$, and thus the probabilty of the
next element being 0 is $\theta$.  That is, there's a probability of
$\theta$ of generating the same element as the last element.

Here is a Stan program that generates the first $M$ entries of
a Markov chain over outputs 0 and 1, with probability $\theta$ of generating
the same output again.

```{.stan filename="stan/markov_autocorelation.stan"}
data {
  int<lower=0> M;
  real<lower=0, upper=1> rho;  // prob of staying in same state
}
generated quantities {
  array[M] int<lower=0, upper=1> y;  // Markov chain
  y[1] = bernoulli_rng(0.5);
  for (m in 2:M) {
    if (y[m - 1] == 1) {
      y[m] = bernoulli_rng(rho);
    } else {
      y[m] = bernoulli_rng(1 - rho);
    }
  } 
}
```

We can fit the model in Python as we did before and extract the simulated
Markov chain $y$.

```{python}
model = csp.CmdStanModel(
               stan_file = '../stan/markov-autocorrelation.stan')
M = 1000
rhos = []
iterations = []
draws = []
estimates = []
for rho in [0.05, 0.5, 0.95]:
    data = {'M': M, 'rho': rho}
    sample = model.sample(data = data, seed = 123, chains = 1,
                      iter_warmup = 0, iter_sampling = 1,
                      show_progress = False, show_console = False)
    y_sim = sample.stan_variable('y')
    cum_sum = np.cumsum(y_sim)
    its = np.arange(1, M + 1)
    ests = cum_sum / its
    draws.extend(y_sim[0])
    iterations.extend(its)
    estimates.extend(ests)
    rhos.extend(itertools.repeat(str(rho), M))
df = pd.DataFrame({'draw': draws, 'iteration': iterations,
                   'estimate': estimates, 'rho': rhos})
print(f"rho = 0.05: {df['rho' == 0.05]['draws'].iloc[0:10]}")
print(f"rho = 0.5: {df['rho' == 0.5]['draws'].iloc[0:10]}")
print(f"rho = 0.95: {df['rho' == 0.95]['draws'].iloc[0:10]}")
```

```{python}
mydraw(
    pn.ggplot(df, pn.aes(x='iteration', y='estimate',
                  group='rho', color='rho'))
    + pn.geom_line()
    + pn.labs(x = "iteration", y = "estimate")
)
```

With a 0.05 probability of staying in the same state, the Markov chain
exhibits strong anti-correlation in its draws, which tend to bounce
back and forth between 0 and 1 almost every iteration.  In contrast,
the 0.95 probability of staying in the same state means the draws have
long sequences of 0s and 1s.  The 0.5 probability produces independent
draws from the Markov chain.  It is clear that the anti-correlated
chain converges much more quickly and more stably than the independent
chain, which in turn converges to the correct estimate of 0.5 much more
quickly than the correlated chain.

We can replace the conditional in the body of the loop to make the
code more concise with a nested user of the _ternary operator_ as follows.
```stan
    y[m] = bernoulli_rng(y[m - 1] ? rho : 1 - rho);
```
The value of `cond ? e1 : e2` is the value of expression `e1`
if the conition `cond` is rue and `e2` otherwise.


## Monte Carlo integration

Bayesian computation relies on computing integrals corresponding to
expectations.  In this section, we will introduce Monte Carlo methods
for calculating integrals with a simple, textbook example of throwing
darts at a board randomly and using the random locations to estimate the
mathematical constant $\pi$.

The idea is that we are going to take a unit square, with values
represented as pairs in $[0, 2]^2$.  Then we are going to generate
points randomly using a uniform distribution over this square.  For
each point, we are going to calculate whether it falls inside the
half-unit circle circumscribed within the square.  The proportion of
such points gives the proportion of the square's volume taken up by the
circle.  Because the square is $2 x 2$, it has an area of 4, so the
circle has an area of 4 times the proportion of points falling in it.

Here's the Stan code.

```{.stan filename="stan/monte-carlo-pi.stan"}
generated quantities {
  real<lower=-1, upper=1> x = uniform_rng(-1, 1);
  real<lower=-1, upper=1> y = uniform_rng(-1, 1);
  int<lower=0, upper=1> inside = sqrt(x^2 + y^2) < 1;
}
```

The program declares variables `x` and `y` and constrains them to fall
in the interval $[-1, 1]$ and assigns them uniform random values.  The
indicator variable `inside` is set to 1 if the Euclidean length of the
vector $\begin{bmatrix}x & y\end{bmatrix}$ is less than 1 (i.e., it
falls within an inscribed unit circle) and is set to 0 otherwise.  The
Euclidean distance is derived directly using the Pythagorean theorem.

First, we compile and then sample from the model, taking `M = 10_000` draws.
Then we plot the draws.

```{python}
M = 100 # 10_000
model = csp.CmdStanModel(stan_file = '../stan/monte-carlo-pi.stan')
sample = model.sample(chains = 1, iter_warmup = 0, iter_sampling = M,
                      show_progress = False, show_console = False,
                      seed = 123)
x_draws = sample.stan_variable('x')
y_draws = sample.stan_variable('y')
inside_draws = [int(i) for i in sample.stan_variable('inside')]
inside_named_draws = np.array(["out", "in"])[inside_draws]
df = pd.DataFrame({'x': x_draws, 'y': y_draws,
                   'inside': inside_named_draws})
mydraw(
  pn.ggplot(df, pn.aes(x = 'x', y = 'y',
                group='inside', color='inside'))
  + pn.geom_point(size = 0.1)
  + pn.labs(x = 'x', y = 'y')
  + pn.coord_fixed(ratio = 1)
)
```

Next, we take the sample mean of the indicators for being inside the
circle, which produces an estimate of the probability of a point being
inside the circle.  Then, we just have to multiply by the square's
area (4) to get our estimate for $\pi$.

```{python}
Pr_is_inside = np.mean(inside_draws)
pi_hat = 4 * Pr_is_inside
print(f"Pr[Y is inside circle] = {Pr_is_inside:.3f};")
print(f"estimate for pi = {pi_hat:.3f}")
```

The true value of $\pi$ to 3 digits of accuracy is $3.142$, so we are
close, but not exact, as is the nature of Monte Carlo methods.  If we
increase the number of draws, our error will go down.  Theoretically,
with enough draws, we can get any desired precision; in practice, we
don't have that long to wait and have to make due with only a few
digits of accuracy in our Monte Carlo estimates. This is usually not a
problem because statistical uncertainty still dominates our numerical
imprecision in most applications.


# Gotchas for Python programmers

Python uses general-purpose programming language idioms, including
indexing containers from 0 and treating upper bounds in ranges as
exclusive.  This is in contrast to Stan, which follows mathematical
notation for matrix algebra and indexes from 1 (like the
mathematically oriented languages MATLAB and R).

## Indexing from 1

Unlike Python, Stan uses the standard mathematical indexing for
matrices, which is from 1.  If I declare a `vector[3]`, then the valid
indexes are 1, 2, and 3.  If `v` is a vector variable, then `v[0]` is
an indexing error and will throw an exception and log a warning as it
is caught and the resulting MCMC proposal is rejected.

## Inclusive ranges

Unlike Python, Stan uses inclusive ranges, so that `1:3` represents
the sequence 1, 2, 3.  The main disadvantage of inclusive notation is
that it makes disadvantage is now that for `L:U`, the length is `U - L
+ 1`.

Stan's design, which is based around matrix and vector types, derives
more from the mathematical notation for these types, which indexes
from 1, than from the convention in programming languages

## Strong typing

Stan variables are strongly typed.  This means every variable is
assigned a type when it is declared and that type never changes.  Only
values of expressions of the same type of the variable may be assigned
to it.

There are three one dimensional real-valued container types (1D array,
column vectors, and row vectors) and four 2D containers (2D array, 1D
array of vectors, 1D array of column vectors, matrix).  There are
functions to interconvert, but sometimes this must be done manually.

## Block scope

Stan follows the C/C++ convention whereby variables declared within a
block are local to that block.  For example, in this Stan program,
`logit_theta` is only a valid variable within the conditional block;
once control has left the conditional block, `logit_theta` falls out
of scope.

```stan
if (theta < 0.5) {
  real logit_theta = logit(theta);
  ...
}
logit_theta = 1.9; // ILLEGAL: OUT OF SCOPE
```
The solution is to move up the declaration.
```stan
real logit_theta;
if (theta < 0.5) {
  logit_theta = logit(theta);
  ...
}
logit_theta = 1.9;  // OK
```

## Loops are not slow

Unlike in R and Python, loops are fast in Stan because Stan is a
compiled language.  For operations that only involve indexing and
assignment, loops can be faster in Stan than their vectorized
counterparts, because they avoid intermediate allocations.  

However, this is only half of the story.  Whenever functions are
applied to parameters, the operation, its result, and its operands are
recorded in an expression graph.  For operations that involve
functions other than indexing or reshaping operations applied to
parameters, vectorized versions of functions will almost always reduce
peak memory usage and increase speed.

## Whitespace and semicolons

Stan follows C/C++ conventions on whitespace in which any sequence of
whitespace characters (space, tab, new line) are interchangeable
semantically.  Python and R are both space-sensitive in different
ways---Python uses it as a block delimiter and R has an eager
line-based parser.  In Stan, it is OK to continue expressions on
the following line without any special syntactic marker like the
backslash(`\\`) in Python, e.g.,
```stan
real lp = bernoulli_lpmf(1 | theta)
          + normal_lpdf(y[n] | mu[1], sigma[1]);
```
Both R and Python here would try to terminate the assignment to `lp`
after the first line and leave a dangling expression for the second
line.  We recommend following mathematical conventions and breaking
a line before an operator, ideally a term in a chain of additions or
a factor in a chain of multiplications.


# Laplace's problem: Male birth ratio

Bayes formulated a mathematical solution to the _inverse problem_ of
determining a posterior distribution with density $p(\theta \mid y)$
from a prior with density $p(\theta)$, a sampling distribution with
density $p(y \mid \theta)$, and observed data $y$.  But he was not able to
solve the integral presented in the denominator of his theorem and
actually determine the form of the posterior.

## Laplace's data on live births

A decade later, @laplace1774 solved the integral that vexed Bayes and
applied Bayes's ideas to the applied problem of estimating whether a
boy is more likely to be born than a girl. Laplace gathered data on
the sexes of babies in live births in Paris between 1745 and 1770.  

sex | live births
---:|:---
female | 105,287
male | 110,312
: Live births in Paris between 1745 and 1770.


## A Stan program for Laplace's problem

Unlike the first Stan model we saw, which only generates data, the
following Stan program is going to allow us to observe the number of
male births ($y$) and the total number of births ($N$) and estimate
the probability of a male birth ($\theta$) as well as the probability
that boys are more likely to be born than girls ($\theta > 0.5$). The
Stan program to do this is as follows.

```{.stan filename="stan/sex-ratio.stan"}
data {
  int<lower = 0> N;  // number of live births
  int<lower = 0, upper = N> y;  // male births
}
parameters {
  real<lower=0, upper=1> theta;  // chance of boy
}
model {
  theta ~ uniform(0, 1);  // uniform prior
  y ~ binomial(N, theta);  // binomial sampling
}
generated quantities {
  int<lower=0, upper=1> boys_gt_girls = theta > 0.5;
}
```

## Parameter and model blocks

In this Stan program, we see that both the number of total births $N$
and the number of male births $y$ are given as data. Then there are
two additional blocks we did not see in our earlier program, a
_parameters block_, which is used to declare unknown values (here the
male birth rate $\theta$), and a _model block_, which is where we
define our target density, typically factored as a prior and sampling
distribution. The parameters block declares the type of `theta`, which
is a real value constrained to fall in the interval $[0, 1]$. The
model block defines the prior, which we take to be uniform over the
possible values for `theta`. The model block also defines the sampling
distribution, which codes the fact that the observed data `y` was generated
from a binomial distribution with `N` trials and `theta` probability
of a male birth. Finally, we have a generated quantities block that
defines a single binary indicator variable, `boys_gt_girls`. This
variable will take the value 1 if the probability of a boy is greater
than the probability of a girl.

## Sampling from the posterior

When we run a Stan program, what Stan returns is a sequence of $M$
random draws, which are approximately identically distributed
according to the posterior,
$$
\theta^{(1)}, \ldots, \theta^{(M)} \sim p(\theta \mid y)
$$
If we were to take $M \rightarrow \infty$, the draws will converge to
being identically drawn from the posterior. With a large enough finite
$M$, the draws will become numerically indistinguishable from true
draws from the posterior.

Stan uses a _Markov chain Monte Carlo_ (MCMC) algorithm, which can
lead to autocorrelation in the random draws from the posterior.  That
is, the draws are not typically independent, with each draw being
correlated (or anti-correlated) with the previous draw.  This
autocorrelation does not introduce bias into the Monte Carlo
estimates.

Stan uses a dynamically adaptive form of Hamiltonian Monte Carlo (HMC)
known as the no-U-turn sampler (NUTS).  NUTS can be hyper-efficient in
the sense of generating anti-correlated draws that can lead to more
efficient Monte Carlo estimates than independent draws in some
cases.

We fit Laplace's model by compiling the model, constructing a
dictionary for the data, and then calling the `sample` method on the
compiled model with the dictionary. We call the sample method with
1,000 warmup iterations and 10,000 sampling iterations; we are taking
so many draws in order to draw smooth histograms later.

```{python}
model = csp.CmdStanModel(stan_file = '../stan/sex-ratio.stan')
boys = 110312
girls = 105287
data = {'N': boys + girls, 'y': boys}
M = 100 # 10_000
sample = model.sample(data = data, seed = 123,
                      iter_sampling = M, iter_warmup = 1000,
                      show_progress = False, show_console = False)
```

As before, we proceed by first extracting the draws for the variables
`theta` and `boys_gt_girls`.

```{python}
theta_draws = sample.stan_variable('theta')
boys_gt_girls_draws = sample.stan_variable('boys_gt_girls')
```

We can plot a histogram of approximate draws $\theta^{(m)} \sim
p(\theta \mid y)$ from the posterior to give us a sense of the
value of $\theta$ and its uncertainty given our observed data $y$.

```{python}
mydraw(
  pn.ggplot(pd.DataFrame({'theta': theta_draws}),
            pn.aes(x = 'theta')) +
  pn.geom_histogram(color='white') +
  pn.labs(x = 'θ') +
  pn.theme(axis_text_y = pn.element_blank(),
           axis_title_y = pn.element_blank(),  
           axis_ticks_major_y = pn.element_blank())
)
```

All of the draws have a value for $\theta$ between 0.50 and 0.52.  In
the next sections, we will see how to use these draws to estimate a
single value for $\theta$ as well as to compute probabilities, such as
the probability that $\theta > 0.5$ or $\theta > 0.51$.


## Bayesian point estimates

In Bayesian terms, a _point estimate_ for a parameter $\Theta$
conditioned on some observed data $Y = y$ is a single value
$\hat{\theta} \in \mathbb{R}^D$ that in some way summarizes the
posterior $p(\theta \mid y)$.

### Posterior mean estimator

The most common Bayesian point estimate for a parameter is the
posterior mean,
\begin{align}
\widehat{\theta}
&= \mathbb{E}[\theta \mid y]
\\[6pt]
&= \int_{\Theta} \theta \cdot p(\theta \mid y) \, \textrm{d}\theta
\\[6pt]
&= \lim_{M \rightarrow \infty} \, \frac{1}{M} \sum_{m=1}^M \theta^{(m)}
\\[6pt]
&\approx \frac{1}{M} \sum_{m=1}^M \theta^{(m)},
\end{align}
where in the last two lines, each draw is distributed approximately
according to the posterior, $\theta^{(m)} \sim p(\theta \mid y)$.

For Laplace's model, the estimate for the male birth rate $\theta$
conditioned on the birth data $y$ is calculated as the sample mean
for the extracted draws for `theta`.

```{python}
theta_hat = np.mean(theta_draws)
print(f"estimated theta = {theta_hat:.3f}")
```



### Posterior median estimator, quantiles, and intervals

A popular alternative Bayesian point estimate is the _posterior
median_, $\theta^+$.  The median is such that for each dimension
$d \in 1{:}D$,
$$
\Pr[\Theta_d \leq \theta^+_d] = \frac{1}{2}.
$$

The posterior median can be calculated by taking the posterior
median of the draws,
```{python}
theta_plus = np.median(theta_draws)
print(f"estimated (median) theta = {theta_plus:.3f}")
```
Because our posterior distribution is nearly symmetric with Laplace's
data, the posterior median is very close to posterior median.

#### Quantiles

Other posterior quantiles are estimated the same way.  For example,
if we want the posterior 95% quantile, we just take the empirical
95% point in the sorted chain of draws.  For example, here
are the 5% quantile and 95% quantile for Laplace's posterior,
calculated with empirical quantiles.
```{python}
quantile_05 = np.quantile(theta_draws, 0.025)
quantile_95 = np.quantile(theta_draws, 0.975)
print(f"""0.05 quantile = {quantile_05:.3f};
  0.95 quantile = {quantile_95:.3f}""")
```

#### Posterior intervals

Together, the 5% quantile and 95% quantile give us the bounds of
our 90% _central probability interval_.  That is, it's the interval
containing 90% of the posterior probability mass, with half of the
remaining mass (5%) taking on higher values and the other half of the
remaining mass (5%) taking on lower values.
```{python}
print(f"central 90% posterior interval for theta")
print(f"    = ({quantile_05:.3f}, {quantile_95:.3f})")
```
Other intervals are computed in the exact same way.

### Posterior mode estimator

A popular non-Bayesian point estimate is the _posterior mode_
$\theta^*$, defined as the point with the highest posterior density,
$$
\theta^* = \textrm{arg max}_\theta \ p(\theta \mid y).
$$
Although the posterior mode can be calculated in Stan using 
optimization rather than sampling, we do not consider it here.
The posterior mode is sometimes called the _maximum a posteriori_
(MAP) estimator.

### Estimation error and bias

The _error_ of an estimate is its difference from the true value,
$$
\textrm{err} = \hat{\theta} - \theta.
$$
Our estimate $\hat{\theta}$ is implicitly a function of the data $y$
and so is $\textrm{err}$, so we can make this explicit and write
$$
\textrm{err}(y) = \hat{\theta}(y) - \theta.
$$

The _bias_ of an estimator is defined as its expected error,
\begin{align}
\textrm{bias}
&= \mathbb{E}[\textrm{err}(Y)]
\\[6pt]
&= \mathbb{E}[\hat{\theta}(Y) - \theta]
\\[6pt]
&= \int_Y \hat{\theta}(y) - \theta \ \textrm{d}y.
\end{align}

The posterior mean is a popular Bayesian estimator for two reasons.
First, it is an _unbiased_ estimator in the sense of having zero
bias.  Second, it has the minimum expected square error among
unbiased estimators, where the _squared error_ of an estimate is
defined by
$$
\textrm{err}^2(y) = (\hat{\theta}(y) - \theta)^2.
$$

The posterior median has the pleasant property of being robust to
outliers. The posterior mode, or maximum a posteriori estimate, is at
least consistent in the sense of converging to the true value as the
data size grows.

We will concentrate on posterior means in this quick introduction to
Bayes and Stan.


### (Markov chain) Monte Carlo error and effective sample size

The Markov chain we use to sample is itself a random variable.
Re-running the sampler will produce slightly different results due to
Monte Carlo error (the error introduced by using only a finite
sample of $M$ draws).  

Stan reports Markov chain Monte Carlo _standard error_ along with
its estimates of the mean.  The MCMC standard error is for a scalar
parameter $\theta_d$ is defined to be
$$
\textrm{mcmc-se}
= \frac{\textrm{sd}[\theta_d \mid y]}{N^{\textrm{eff}}},
$$
where the numerator is the standard deviation of the parameter
$\theta_d$ in the posterior and $N^{\textrm{eff}}$ is the _effective
sample size_.  In the usual central limit theorem, the number of
samples appers here.  The effective sample size for a sample of size
$M$ is defined to be
$$
N^{\textrm{eff}}
= \frac{M}{\textrm{IAT}},
$$
where $\textrm{IAT}$ is the _integrated autocorrelation time_.
The IAT can be thought of as the interval between effectively
independent draws in our Markov chain.  If we have low
autocorrelation, $\textrm{IAT}$ will be close to 1 and if the
autocorrelation is higher, it can be much higher.  If the
$\textrm{IAT}$ is much higher than 100, it can become difficult
to estimate.  If the
autocorrelation is negative, the $\textrm{IAT}$ is less than 1 and the
effective sample size is larger than the number of draws.  Thus
$N^{\textrm{eff}}$ is the number of independent draws that would
lead the same error as our correlation draws using a Markov chain.

## Estimating event probabilities

Laplace wasn't looking for a point estimate for $\theta$.  He wanted to
know the probability that $\theta > \frac{1}{2}$ after observing $y$
male births in $N$ trials.  In the notation of probability theory,
he wanted to estimate an event probability.

A subset of parameters is known as an _event_. We can convert
conditions on parameters into events. For example, the condition
$\theta > \frac{1}{2}$ can be turned into the event
$$
A = \left\{ \theta \in \Theta : \theta > \frac{1}{2} \right\}.
$$
Events are what are assigned probabilities by a _measure_ in probability
theory. Given a probability measure, the probability of the event $A$,
that the rate of boy births is higher than girl births, will be well defined.
Because we can convert conditions to events, we will be
sloppy and treat the conditions as if they were events. This allows us
to write $\Pr[\Theta > \frac{1}{2} \mid N, y]$ for the probability of
the event $\Theta > \frac{1}{2}$.

Technically, we will need to use the _indicator function_ $\textrm{I}$,
which maps propositions like $\theta > \frac{1}{2}$ into the value 1
if they are true and 0 if they are false.  

### Event probabilities via indicators

Event probabilities are defined as posterior conditional expectations
of indicator functions for events.
\begin{align}
\Pr[\Theta > 0.5 \mid N, y]
&= \mathbb{E}\!\left[\textrm{I}[\Theta > 0.5] \mid N, y\right]
\\[8pt]
&= \int_{\Theta} \textrm{I}(\theta > 0.5) \cdot p(\theta \mid N, y) \, \textrm{d}\theta
\\[8pt]
&\approx \frac{1}{M} \sum_{m=1}^M \textrm{I}(\theta^{(m)} > 0.5),
\end{align}
where we assume $\theta^{(m)} \sim p(\theta \mid N, y)$ is distributed
according to the posterior for $m \in 1{:}M$. Following physics
conventions, we use square brackets for functors (functions that apply
to functions); that means we write $\textrm{I}[\cdot]$ when we apply
the indicator function to a random variable and 
$\textrm{I}(\cdot)$ when we apply it to a primitive scalar.

### Events as indicators in Stan

In Stan, we code the value of the indicator function directly
and assign it to a variable in the generated quantities block.
```stan
generated quantities {
  int<lower=0, upper=1> boys_gt_girls = theta > 0.5;
}  
```
Conditional expressions like `theta > 0.5` take on the value
1 if they are true and 0 if they are false.  As is conventional in
languages such as C++, the indicator function itself isn't written.

### The answer to Laplace's question

The posterior mean of the variable `boys_gt_girls` is thus our estimate
for $\Pr[\theta > 0.5 \mid N, y]$.  It is essentially 1.  Printing to
15 decimal places, we see

```{python}
Pr_boy_gt_girl = np.mean(boys_gt_girls_draws)
print(f"estimated Pr[boy more likely] = {Pr_boy_gt_girl:.15f}")
```

The value of 1 returned as an estimate brings up the important
problem of numerical precision.  As we can see from the histogram,
all of our sampled values for $\theta$ are greater than $\frac{1}{2}$.

Laplace calculated the result analytically, which is
$$
\Pr\!\left[\theta > \frac{1}{2}\right] \approx 1 - 10^{-27}.
$$
Thus we would need an astronomical number of posterior draws
before we would generate a value of $\theta$ less than $\frac{1}{2}$.
As given, the answer of 1.0 is very close to the true answer and well
within our expected Monte Carlo error.


### MCMC summary statistics from Stan

With Stan, we can print a summary for the variable $\theta$ in the
posterior, which reports all of these values.  We just call The
`.summary()` function on the sample.  

```{python}
sample.summary(sig_figs = 3)
```

The rows are for our random variables, with `lp__` indicating the
unnormalized log density defined by the Stan program (which includes
change of variable adjustments for constrained parameters).  The
other two rows are for variables defined in the Stan program, `theta`,
and `boys_gt_girls`.  The number of significant figures used in the
results can be controlled in the summary function, as can the
quantiles being reported.

The first column reports the posterior mean, and agrees with our
earlier calculations for both variables.  The second column is
the Monte Carlo standard error (based on an estimated effective
sample size) and a posterior standard deviation estimate.  The
next three columns are quantiles we computed earlier, and they also
agree with our calculations.  Next is the effective sample size,
which can vary from variable to variable, and the effective sample
size rate (per second).  The final column reports $\widehat{R}$, which we
discuss in the next section.


# Warmup and convergence monitoring

When running Markov chains, we want to make sure that we have moved far
enough that our draws are approximately from the posterior.  A
standard way to monitor convergence is to start multiple Markov chains
at different initializations (ideally chosen from a diffuse
initialization like a draw from the prior) and measure whether they
are producing draws from the same distribution.

## Warmup

Stan performs a number of _warmup iterations_ during which time it
tries to find the region in which it should be sampling and then adapt
a good step size and estimate the posterior covariance (just the
diagonal with the variances by default).  The step size is used in the
Hamiltonian dynamics simulator and the (co)variance is used as a
preconditioned.

Warmup converges when the step size and posterior covariance estimates
no longer change.  With multiple chains, it's possible to test that
they have all converged to the same step size and covariance estimate.
Typically, we don't bother doing this and just measure our end goal
directly, which is whether we are getting reasonable posterior draws
after warmup.

Warmup doesn't form a single coherent Markov chain because it uses
memory to adapt.  Once Stan starts sampling, the result is a Markov
chain.  All of our posterior analysis will be with draws from the
Markov chain, not from warmup.  We can save and extract the warmup
draws to investigate the behavior of warmup.

## Potential scale reduction and $\widehat{R}$

Stan uses the _potential scale reduction_ statistic $\widehat{R}$
(pronounced ``R hat'').  Given a sequence of Markov chains, Stan
splits them in half to make sure the first halfs and second halfs of
the chain agree, then calculates variances within each chain and
across all chains and compares.  The statistic $\widehat{R}$ converges
to 1 as the Markov chains converge to the same distributions.

## Practical guidelines

A simple rule of thumb is to run four chains as long as necessary with
equal numbers of warmup and sampling iterations until the sample has
$\hat{R} < 1.01$.  Running more warmup iterations is important because
sampling will not be efficient if warmup has not converged and it at
most uses a few more iterations compared to simply extending the
number of sampling iterations.

We also want to make sure that our effective sample size is at least
25 per chain.  It is not that we need so many draws for inference, but
that we do not trust our effective sample size estimator if it is much
lower.   One way to check that the ESS estimator is OK is to double
the number of draws and make sure that the ESS also doubles.  If it
doesn't, it's a sign that the first ESS estimate is unreliable.

## Running chains concurrently

You can set the number of chains to run using the `chains` argument of
the `sampling()` method and you can set the maximum number of chains
to execute concurrently using `parallel_cores` (which defaults to 1,
sequential execution).  If you set the maximum number of parallel
chains to be too low, CPU resources are potentially unused.  If you
set the number too high, then either CPU or memory can bottleneck
performance and cause it to be slower than running with fewer chains
in parallel.  The only advice I can give here is to experiment.  In
personal projects on our own hardware, the goal is usually the largest
effective sample size in the minimum amount of time.  Perhaps we want
to leave enough CPU power left over to continue to work on other
things, or maybe not.  In a server setting, memory usage, latency,
throughput, and I/O need to be balanced more carefully.

# A/B testing

A common application of statistics is to compare two things, such as
the effectiveness of a new drug versus the current drug used to treat
a condition on the one hand, or the effectiveness of two different ad
presentations in getting users to click through.  This is usually called
_A/B testing_ in a nod to comparing a hypothetical option A and option B.

Let's consider some Mexican restaurants in New York City, Downtown
Bakery II in the East Village, Taqueria Gramercy in Gramercy, and
La Delicias Mexianas in Spanish Harlem.  Here's the number of reviews
and 5-star reviews for these restaurants on the web site Yelp.

| name | 5-star reviews | total reviews |
|:--|--:|--:|
| Downtown Bakery II | 141 | 276 | 0.511 |
| Taqueria Gramercy | 84 | 143 | 0.587 |
| La Delicias Mexicanas | 41 | 87 | 0.471 |

We can estimate a few things.  First, we can estimate the probability
that each restaurant really is a 5-star restaurant.  We will parameterize
this directly with a rate of 5-star reviews parameter for each restaurant.
Then we can rank the restaurants based on their probability of being a
5-star restaurant.  What does it mean to "be a 5-star restaurant" in
this sense?  It means getting 5-star reviews from Yelp reviewers.
Our model is going to treat the reviewers as _exchangeable_ in the
sense that we don't know anything to distinguish them from one
another.  Now whether this notion of 5-star restaurant is useful will
depend on how similar the reader is to the population of reviewers.

We will assume that the number of 5-star reviews for a restaurant
$k \in 1{:}K$ depends on its underlying quality $\theta_k \in [0, 1]$,
$$
n_k \sim \textrm{binomial}(N_k, \theta).
$$
Here $N_k \in \mathbb{N}$ is the number of reviews for restaurant $k$
and $n_k \in 0{:}N_k$ the number of 5-star reviews.  We will further
the probabilities of 5-star reviews have a beta distribution,
$$
\theta_k \sim \textrm{beta}(\alpha, \beta).
$$
In a beta distribution, the sum $\alpha + \beta$ determines how much
to regularize estimates toward $\alpha / (\alpha + \beta)$, with
$\alpha = \beta = 1$ providing a uniform distribution.

For inference, we will be interested in the posterior distribution
$p(\theta \mid N, n)$ of 5-star review probabilities. This gives us
the posterior density of the restaurants' probability of receiving a
5-star review. With this posterior, we can rank restaurants based on
their probability of receiving a 5-star review and calculate the
probability that each is the best restaurant,
$$
\textrm{Pr}[\theta_k = \max(\theta) \mid n, N].
$$

## Stan model for A/B testing

We will assume there are a total of $K$ items being compared.  In
traditional A/B testing, $K = 2$, but our example uses $K = 3$ and our
code works for any $K$.  We define the indicator variable for our
event probability estimate in the generated quantities block.

```{.stan filename='stan/ab-test.stan'}
data {
  int<lower=0> K;
  array[K] int<lower=0> trials;
  array[K] int<lower=0> successes;
  real<lower=0> alpha;  
  real<lower=0> beta;  
}
parameters {
  array[K] real<lower=0, upper=1> theta;
}
model {
  successes ~ binomial(trials, theta);
  theta ~ beta(alpha, beta);
}
generated quantities {
  array[K] int<lower=0, upper=1> is_best;
  for (k in 1:K) {
    is_best[k] = theta[k] == max(theta);
  }
}
```

We have coded the data for $K$ items directly in terms of number of
trials and number of successes.  We have also supplied the prior for
the probabilities of success as data as `alpha` and `beta`.  The model
is the same binomial as we had before, except now the likelihood and
priors are vectorized.  In general, Stan is able to take something
like the binomial distribution, which has an integer number of trials,
integer number of successes, and a scalar success probability and
take vectors for all of these.  What we have written above is identical
to what we would get with a loop,

```stan
  for (k in 1:K) {
    successes[k] ~ binomial(trials[k], theta[k])
  }
```

The vectorization of `theta` is different in that only `theta` is an
array, whereas `alpha` and `beta` are scalars.  The sampling statement for
`theta` is equivalent to

```stan
  for (k in 1:K) {
    theta[k] ~ beta(alpha, beta);
  }
```

Because `alpha` and `beta` are scalars, they are not indexed.  Rather,
they are _broadcast_, meaning that the same `alpha` and `beta` is
reused for each dimension of `theta`.

So now let's call and fit this model and print the summary.  We
are setting $\alpha = \beta = 2$, which is equivalent to setting
them equal to 1 and adding 2 trials and 1 success to the data for
each restaurant.

```{python}
model = csp.CmdStanModel(stan_file = '../stan/ab-test.stan')
data = {'K': 3, 'trials': [276, 143, 87],
        'successes': [141, 84, 41],
        'alpha': 2, 'beta': 2}
sample = model.sample(data = data, seed = 123,
                      show_progress = False, show_console = False)
sample.summary(sig_figs = 2)
```

First, we make sure that our $\widehat{R}$ values are less than 1.01
and that our $N^{\textrm eff}$ are all greater than 25 per chain (4
chains here with default settings).  In fact, we can see that our
sampling is nearly as efficient as if we had independent posterior
draws.  The probability of a 5-star review is our value for `theta`,
which are 51% for Downtown Bakery, 59% for Taqueria Gramercy, and 47%
for La Delicias.  Even so, we see that the probability that Taqueria
Gramercy is the most likely to have their next review be a 5-star
review, it's still only 90% that's the case.  This is because binomial
data is weak and we only have observations in the hundreds.  (Editor:
Don't listen to Yelp---La Delicias is the best of these restaurants by
far.)

# Stan's execution model

Stan programs consist of several blocks.  Here is a summary of when
the blocks are executed and what they do.

| block | executed | behavior |
|:--|:--|:--|
| functions | as needed | user-defined function definitions |
| data | once | read data to construct model |
| transformed data | once | define transformed data |
| parameters | once / log density | constrain parameters (w. Jacobian) |
| transformed parameters | once / log density | define transformed parameters |
| model | once / log density | evaluate model log density |
| generated quantities | once / draw | define generated quantities |

## Data and transformed data

The _data_ block contains only variable declarations. The variables declared in the data block are
read once at data load time and then remain constant throughout
execution.

The _transformed data_ block contains variable declarations and
definitions.   is executed once, after the data is read is
in, to define the transformed data variables.  It defines functions
of the data, such as standardized predictors, constants to use for
priors, etc.

All variable declarations at the block level must declare types and
sizes (which may be data dependent).  Local variables within blocks
are declared without sizes.

Constraints in the data block are evaluated at the end of the block's
execution.  Any violations throw exceptions which terminate execution.

## Parameters and transformed parameters

The _parameters_ block declare the variables over which the model is
defined. It contains only variable declarations with sizes. When
executed, it is supplied with concrete parameter values, which it
transforms to the unconstrained scale based on the declared
constraints (if any).

Constraints on parameters are used to define a transform of the
constrained space to $\mathbb{R}^D$. For example, a variable declared
with a `<lower=0>` constraint is log-transformed to an unconstrained
variable, whereas a variable declared as a covariance matrix is
Cholesky factored and then a log transform is applied to the diagonal
elements to render an $N \times N$ matrix as a vector in
$\mathbb{R}^{\binom{N}{2}}$. It is critical that parameters are
declared with all necessary constraints so that the model has support
(i.e., non-zero density, finite log density) over the entire
transformed space.

The _transformed parameters_ block defines functions of parameters and
data. Any constraints declared on transformed parameters are validated
at the end of the block's execution.  If the constraints are violated,
an exception will be thrown, which typically causes the current
proposal to be rejected.

## Model

The _model_ block defines the log density and is evaluated once per
log density evaluation. The log density starts with any components
added by the log Jacobian determinants of the inverse transforms of
the parameters from the unconstrained to the constrained space. The
model executes over constrained parameters. The current state of the
log density is available through the variable `target`, which can be
updated directly, e.g.,

```stan
target += -0.5 * x^2;
```

The current value of the target is available as `target()` and
printing it can be useful for debugging.

The other statement that affects the eventual target log density
returned are _sampling statements_, e.g., 

```stan
x ~ normal(0, 1);
```

Sampling statements are shorthand for target increment statements, e.g.,

```stan
target += normal_lupdf(x | 0, 1);
```

The vertical bar is used to separate outcome variates from the
parameters.  The notation `lpdf` is used for log density functions,
`lpmf` for mass functions, and `lupdf` and `lupmf` for their
forms that drop constants.  Unless the normalizing constants are
needed, it is more efficient to use the `lupdf` and `lupmf` forms
either explicitly or implicitly using sampling statements.

## Generated quantities

The _generated quantities_ block is evaluated once per draw rather
than once per log density evaluation. With algorithms like Hamiltonian
Monte Carlo, several, dozens, or even hundreds of log density
evaluations are used for each draw. The further advantage of generated
quantities is that they do not require differentiation. They may also
use pseudo-random number generation. Any constraints are evaluated at
the end, but exceptions do not cause rejections, just potential
warnings and potentially undefined (not-a-number) values.

Generated quantities are not part of the model and do not affect
sampling for the parameters at all.  As such, they are typically used
for posterior predictive inference, as we have done above and will
continue to do below.

## User-defined functions

Users can define functions in the _functions_ block. These can be
applied anywhere in a Stan program, just like built-in functions. In
addition to ordinary mathematical functions, users can define several
types of special-purpose functions with Stan-specific behavior.

### User-defined density and mass functions

Defining a function with any of the suffixes `lpdf`, `lupdf`, `lpmf`,
or `lupmf` defines a density or mass function.  The first argument is
the variate and the remaining arguments are the parameters.  These
functions may then be used with vertical bar notation and in sampling
statements. 

```stan
functions {
  real foo_lpdf(real y, real theta) {
    return normal_lpdf(y | 0, exp(-theta))
  }
}
```

and then the model can contain target increment statements using the
vertical bar,

```stan
  target += foo_lpdf(z | phi);
```
or just directly use sampling statements
```stan
  z ~ foo(phi);
```

Stan validates that the first argument of an `lpdf` or `lupdf` is
real-valued and that the first argument of an `lpmf` or `lupmf` is
integer-valued.

### User-defined random number generators


generator.  Only functions defined with `_rng` suffixes will be able
to call other functions with `_rng` suffixes and they will only be
able to be used in Stan programs in the transformed data and generated
quantities blocks (i.e., the blocks that are not part of the model
definition and hence do not need to be differentiated).

For example, a simple way to generate chi-squared random variates is
to literally sum a bunch of squared normal variates (do not do this in
a real program as there is a built-in chi-square `_rng` function).

```stan
real simple_chi_sq_rng(int n) {
  real y = 0;
  for (i in 1:n) {
    y += normal_rng(0, 1)^2;
  }    
  return y;
}
```

### Modifying the target

Functions that use the suffix `_lp` can access and modify the log
density.  This means they can only be used in transformed parameters
or model blocks.  For example, the following function implements what
Stan does implicitly for a variable declared with a `lower=0` constraint.

```stan
real pos_constrain_lp(real v) {
  target += v;  // change of variables adjustment
  return exp(v);  // change of variables
}
```

## Automatic differentiation

Stan uses _automatic differentiation_ to define gradients of the log
density function.  This is done by building up the complete
_expression graph_ of the log density being defined by the Stan model.
A concrete value for unconstrained parameters is supplied, these are
constrained and the log absolute determinant of the Jacobian of
the unconstraining transformed is added to the expression graph.
Then each operation involving parameters is added to the expression
graph.  The root is the final log density value, which is the value
of `target`.  The leaves are the unconstrained parameters.  A
reverse pass over this expression graph propagates the partial
derivatives from the log density value down to the unconstrained
parameters using the chain rule (it applies in the reverse order of
the expression construction, which is a topological sort of the
directed graph).

# Regression and prediction

In this section, we will go over simple regression models in Stan and
see how to generate predictions for new items based on fitted
parameters.

## Fisher's iris data set

We will analyze Fisher's classic iris data set, which provides sepal
and petal length and width (in centimeters) as well as the species of
iris.  First we read it in, then we will plot petal width versus petal
length, with species indicated by color.

```{python}
df = pd.read_csv('../stan/iris-data.csv')
mydraw(
  pn.ggplot(df, pn.aes(x='petal_width', y='petal_length',
                       color='species')) +
  pn.geom_point() +
  pn.labs(x = "petal width (cm)", y = "petal length (cm)")
)
```

The three species are of very different sizes, but there is a roughly
linear relation between petal width and petal length that fits the
entire
data.

The width values all have exactly one decimal place of accuracy, which
means they were recorded to the nearest millimeter. There are models
that deal with this kind of measurement quantization, but we will not
consider it now and proceed as if there were no rounding of our
measurements.

## Linear regression

A linear regression models one measurement as a linear function of
another measurement.  In this example, we will model an iris flower's
petal length as a linear function of its petal width.  If we let $x_n$
be the petal width for flower $n$ and let $y_n$ be the petal length.
A linear regression says that we expect $y_n$ to be a linear function
of $x_n$, or roughly, the expected value of $y_n$ will be $\alpha +
\beta \cdot x_n$ for some intercept $\alpha$ and slope $\beta$.  A
linear regression allows an inexact relationship where the linear
relation only holds in expectation.  In real observations, there will
be _error_ due to either measurement or a failing in the scientific
model (which is the very simple linear relation).  

We can introduce a variable $\epsilon_n$ for the difference between a
petal's length and its expected length given the linear relationship, 
$$
y_n = \alpha + \beta \cdot x_n + \epsilon_n.
$$
It is traditional to assume the error is normally distributed with a
zero mean and scale $\sigma > 0$, 
$$
\epsilon_n \sim \textrm{normal}(0, \sigma).
$$
We can rearrange terms and write this in a fashion that will mirror
how it's coded in Stan, 
$$
y_n \sim \textrm{normal}(\alpha + \beta \cdot x_n, \sigma).
$$
In this form, it's clear that the linear prediction is the expectation
for the random variable $Y_n$ and the standard deviation is the scale,
$$
\mathbb{E}[Y_n] = \alpha + \beta \cdot X_n
\qquad
\mathrm{sd}[Y_n] = \sigma.
$$


Here is a simple Stan model to regress petal length on petal width;
we replace $x$ with `petal_width` and $y$ with `petal_length`.

```{.stan filename='../stan/iris-petals.stan'}
data {
  int<lower=0> N;
  vector<lower=0>[N] petal_width;
  vector<lower=0>[N] petal_length;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  petal_length ~ normal(alpha + beta * petal_width, sigma);
  alpha ~ normal(0, 5);
  beta ~ normal(0, 5);
  sigma ~ lognormal(0, 1);
}
```

The data block says there are $N$ observations of petal width and
length.  The widths and lengths are declared to have type (column)
`vector`, with a constraint `lower=0` and size (number of rows) `N`.
We are declaring these types as vectors because we are going to apply
matrix multiplication and addition to them.

The model parameters are coded in the parameter block.  The lower
bound for `sigma` is required here as the normal distribution is only
defined for positive values of `sigma`.  Stan requires every parameter
value that satisfies the declared constraints to be in the _support_
of the density, which is the set of values for which the density is
non-zero (equivalently, the log density is finite).

The model codes the regression following the math, but without the
subscripts.  What's going on is that the sampling statement for
`petal_length` is vectorized.  The variables `petal_length` and
`petal_length` are both vectors of size `N`, whereas `alpha`, `beta`,
and `sigma` are all scalars.  Working outward, `beta * petal_width` is
a scalar times a vector, which is defined in the usual way as a vector
whose `n`th entry is `beta * petal_width[n]`.  We then add `alpha` to
that result (multiplication binds more tightly than addition and as in
mathematical writing, we drop all unnecessary parentheses).  This is
defined by _broadcasting_ so that `alpha` acts like a vector of size
`N` and addition is defined in the usual way for vectors.  As a
result, `alpha + beta * petal_width` is a vector of size `N`, whose
`n`th element is `alpha + beta * petal_width[n]`.  Now we have a
vector for the location parameter of a normal distribution, a scalar
scale (`sigma`), and a vector outcome (`petal_length`).  This works by
broadcasting `sigma` to use for each `n`.  The end result is
equivalent to the following, but much more compact and efficient.
```stan
for (n in 1:N) {
  petal_length[n] ~ normal(alpha + beta * petal_width[n], sigma);
}
```
The rest of the model is priors for the regression coefficients and error scale.
The coefficients are unconstrained, but the error scale is constrained
to be positive, so we use a lognormal distribution (we could have also
used a half normal and we should if scales near zero are possible).

We can then compile and fit the model and display the resulting
fit for `alpha` and `beta` as a scatterplot.

```{python}
def iris_data_frame():
    return pd.read_csv('../stan/iris-data.csv')

def iris_data():
    df = iris_data_frame()
    N = df.shape[0]
    petal_width = df['petal_width']
    petal_length = df['petal_length']
    species = 1 + pd.Series(df['species']).astype('category').cat.codes
    num_species = 3
    data = {'N': N,
            'K': num_species,
            'species': species,
            'petal_width': petal_width,
            'petal_length': petal_length,}
    return data
  
model = csp.CmdStanModel(stan_file = '../stan/iris-petals.stan')
sample = model.sample(data = iris_data(), seed = 123,
                      show_progress = False, show_console = False)
sample.summary(sig_figs = 2)
```
We have added both the species as a string and the species as an
index (using pandas's notion of a factor).


This summary doesn't give us a good feeling for the uncertainty in the
linear relationship.  To do that, we will plot multiple draws from the
posterior along with the data.

```{python}
alpha_draws = sample.stan_variable('alpha')
beta_draws = sample.stan_variable('beta')
plot =  pn.ggplot(df, pn.aes(x='petal_width', y='petal_length',
                             color='species'))
plot = plot +  pn.geom_point()
plot = plot +  pn.labs(x = "petal width (cm)", y = "petal length (cm)")
for a, b in zip(alpha_draws[0:20], beta_draws[0:20]):
    plot = plot + pn.geom_abline(intercept = a, slope = b,
                                 alpha = 0.5, size = 0.2)
mydraw(
  plot
)
```

In order to fit all three groups of data, the plot doesn't do such a
great job of fitting any of them---the larger *iris setosa* instances
haver overestimated petal lengths, whereas they are overestimated for
*iris versicolor*.

We didn't include the error scale, but it's roughly 0.5.  That means
it won't be uncommon to get errors greater than 1 or less than -1.
For *iris setosa*, this could easily result in predictions with
negative lengths!

## Robust regression

Although we won't consider this model in detail, Stan's plug-and-play
design makes it very simple to convert our linear regression into a
_robust regression_ by swapping out the normal error model for a
Student-t error model,
```stan
  petal_length ~ student_t(dof, alpha + beta * petal_width, sigma);
```
We have just used `dof` for the degrees of freedom variable, but
setting it at a value like `4` provides wider tails and setting it at
`1` produces the very wide-tailed Cauchy distribution, which does not
even have a finite mean or variance.


## Log-linear regression

We'll fix both problems in Stan. First, we'll include a regression per
species. Second, we'll convert to the log scale so that our predicted
sizes remain positive.


## Posterior predictive inference

Now let's say we have a new observation where we know the petal width
and want to predict its length.  Mathematically, to make a prediction
for a new item, we use _posterior predictive inference_.

First, let's consider evaluating the log density of a new petal's
length ($\tilde{y}$) given its observed width ($\tilde{x}$), having
observed our original data $x$ and $y$.  In Bayesian inference,
we evaluate the _posterior predictive distribution_, where we
let $\Theta$ be our random parameters with realization $\theta$,
\begin{align}
p(\tilde{y} \mid \tilde{x}, x, y)
&= \mathbb{E}[p(\tilde{y} \mid \tilde{x}, \theta) \mid x, y]
\\[6pt]
&= \int_{\Theta} p(\tilde{y} \mid \tilde{x}, \theta) \cdot p(\theta \mid x, y) \, \textrm{d}\theta
\\[6pt]
&\approx \frac{1}{M} \sum_{m=1}^M \, p(\tilde{y} \mid \tilde{x}, \theta^{(m)}),
\end{align}
where $\theta^{(1)}, \ldots, \theta^{(m)} \sim p(\theta \mid x, y)$ are draws from the posterior.
This is just marginalizing out the parameters.  It can help to break the
integral down into the two components of uncertainty, sampling uncertainty
due to our sampling distribution and posterior uncertainty in the values of our parameters,
$$
\int_{\Theta}
\underbrace{p(\tilde{y} \mid \tilde{x}, \theta)}_{\textrm{sampling uncertainty}} 
\cdot
\underbrace{p(\theta \mid x, y)}_{\textrm{estimation uncertainty}}
\, \textrm{d}\theta
$$
In our case, $\theta = \begin{bmatrix}\alpha & \beta & \sigma\end{bmatrix}$ and the sampling
distribution is
$$
p(\tilde{y} \mid \tilde{x}, \alpha, \beta, \sigma)
=
\textrm{normal}(\tilde{y} \mid \alpha + \beta \cdot \tilde{x}, \sigma).
$$
Even if we know the parameter values $\alpha$, $\beta$, and $\sigma$ exactly, we can
only predict $\tilde{y}$ to within $\epsilon$, where $\epsilon \sim \textrm{normal}(0, \sigma)$.
If we plug in a point estimate $\widehat{\alpha}, \widehat{\beta}, \widehat{\sigma}$ for our parameters,
we might get approximate inference that takes into account sampling uncertainty, but not estimation uncertainty, e.g., 
$$
p(\tilde{y} \mid \tilde{x}, x, y)
\approx
p(\tilde{y} \mid \tilde{x}, \widehat{\alpha}, \widehat{\beta}, \widehat{\sigma}).
$$

So far, this only gives us a way to evaluate the log density of a
resulting outcome $\tilde{y}$ given a predictor $\tilde{x}$.  If we
want to simulate possible $\tilde{y}$, we have to make sure to add
sampling uncertainty and draw
$$
\tilde{y}^{(m)} \sim \textrm{normal}\!\left(\alpha^{(m)} + \beta^{(m)} \cdot \tilde{x} \mid \sigma^{(m)}\right),
$$
where $\alpha^{(m)}, \beta^{(m)}, \sigma^{(m)} \sim p(\alpha, \beta,
\sigma \mid x, y)$ are posterior draws.  In general, if we then want to estimate
$\tilde{y}$, we can take posterior means of these values.  In the case here,
where our sampling distribution is symmetric, we can directly compute
expected values of $\tilde{y}$ as $\alpha + \beta \cdot \tilde{x}$, so we
can get by plugging in those values rather than sampling for an estimate of $\hat{y}$.

Let's put all this together into a Stan program.  First, let's make
posterior predictions for some new $\tilde{y}$.  We can do this with a
Stan program that just includes the parameters and the new data and a
generated quantities block.  We can run the program with the old
sample for our parameters to generate our predictions.  We could've also
just included the generated quantities block in the original program.

```{.stan filename="iris-predict.stan"}
data {
  int<lower=0> N_tilde;
  vector<lower=0>[N_tilde] petal_width_tilde;
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
generated quantities {
  vector<lower=0>[N_tilde] E_petal_length_tilde
    = alpha + beta * petal_width_tilde;
  vector<lower=0>[N_tilde] petal_length_tilde
    = to_vector(normal_rng(E_petal_length_tilde, sigma));
}
```

This program declares two new data variables, `N_tilde` for the
number of predicted items, and `petal_width_tilde`, a vector of
petal widths of size `N_tilde`.  

It is important that this program use _exactly the same parameters_ as
the original program.   After our original fit, we read that sample
back in for the parameters, then run generated quantities (and the
transformed parameter block if there is one).

The generated quantities block first calculates the expected petal
length given the petal width and assigns it to the variable
`E_petal_length_tilde`. This code is vectorized so that it calculates
all of the input petal widths at once.  The second variable is
then set by sampling according to the sampling distribution
using the `normal_rng` function.  This function is also vectorized,
but it returns an array, so we convert it to a vector just to keep
all the types the same.


```{python}
data = {'N_tilde': 3,
        'petal_width_tilde': [0.4, 1.75, 3.8]}
model = csp.CmdStanModel(
     stan_file = '../stan/iris-posterior-predictive-sim.stan')
pps_sample = model.generate_quantities(data = data, seed = 123,
                                       previous_fit = sample,
                                       show_console = False)
for i in range(3):
  length_draws = \
      pps_sample.stan_variable('petal_length_tilde')[0:100, i]
  E_length_draws = \
      pps_sample.stan_variable('E_petal_length_tilde')[0:100, i]
  print(f"""{i=}:  mean(E_petal_length_tilde[{i}])
 = {np.mean(E_length_draws):.2f};
  sd(E_petal_length_tilde[{i}]) = {np.std(E_length_draws):.2f}""")
  print(f"""      mean(petal_length_tilde[{i}])
 = {np.mean(length_draws):.2f};
  sd(petal_length[{i}]) = {np.std(length_draws):.2f}\n""")
```

For each of our input petal widths (0.5, 1.75, 3.8), we see the
posterior mean prediction for petal width and its standard deviation
calculated two ways. First, we take the posterior draws for the
expected petal length, `E_petal_length_tilde`, which is just the
linear prediction of petal value. The uncertainty comes only from
estimation uncertainty in $\alpha$ and $\beta$. Second, we take the
posterior draws for petal length, which include an additional normal
error term with scale $\sigma$. The means are roughly the same either
way, but the standard deviation is much higher in the case of
sampling. The second variable, `petal_length_tilde`, includes both
sources of posterior uncertainty, estimation uncertainty and
uncertainty from the sampling distribution.

## Lognormal and transformed data

In this section, we will convert our iris model to the log scale using
the transformed data block.  The only difference to our original model
is that we now use the logs of the petal width and petal length and we
use a lognormal distribution for the regression of the length on the
width.

The _lognormal distribution_ is a simple transform of a normal
distribution with support over positive values.  If $u \sim
\textrm{normal}(\mu, \sigma)$, then $\exp(u) \sim
\textrm{lognormal}(\mu, \sigma)$.  The exponential (inverse) transform
ensures the result is positive.

The other way around, if $v \sim \textrm{lognormal}(\mu, \sigma)$,
then $\log v \sim \textrm{normal}(\mu, \sigma)$.  This will allow us
to transform our Stan program.

```{.stan filename='../stan/iris-petals-log.stan'}
data {
  int<lower=0> N;
  vector<lower=0>[N] petal_width;
  vector<lower=0>[N] petal_length;
}
transformed data {
  vector[N] log_petal_width = log(petal_width);
}
parameters {
  real alpha;
  real beta;
  real<lower=0> sigma;
}
model {
  petal_length ~ lognormal(alpha + beta * log_petal_width, sigma);
  alpha ~ normal(0, 3);
  beta ~ normal(0, 3);
  sigma ~ lognormal(0, 0.6);
}
```

We have introduced a new block for transformed data, in which we have defined
a vector of log petal widths to use as a predictor.  The petal lengths
are already positive, so those stay the same.  The effect is that we
are regressing log petal length on log petal width and the sampling statement
could have been replaced with
```stan
  log(petal_length) ~ normal(alpha + beta * log_petal_width, sigma);
```
The result would be the same up to a constant normalizing term
involving petal length (which is constant) in the lognormal density.


Let's run the model and summarize the results.

```{python}
log_model = csp.CmdStanModel(stan_file =
                               '../stan/iris-petals-log.stan')
log_sample = log_model.sample(data = iris_data(), seed = 123,
                              show_progress = False,
			      show_console = False)
log_sample.summary(sig_figs = 2)
```

One advantage of the log scale is that the regression makes more sense
because $\exp(\alpha + \beta \cdot \log w) = \exp(\alpha) \cdot
w^{\beta}$.  By taking exponents, the intercept becomes a multiplier
and the slope becomes an exponent.  The error also moves to the multiplicative scale, which makes a lot more sense here
as we wouldn't expect the same error on the 0.5cm and 5cm scales.
Now consider our estimated values $\widehat{\alpha} = 1.3$ and $\widehat{\beta} = 0.57$
and $\sigma = 0.16$, which introduce the relationship, with the plus or minus ($\pm$) covering roughly
the central 95% interval of probability mass.
\begin{align}
\textrm{length}
&= \exp(\alpha + \beta * \log(\textrm{width}) \ \pm \ 2 \cdot \sigma)
\\[6pt]
&= 3.67 \cdot \textrm{width}^{0.56} \cdot 1.38^{\pm 1}.
\end{align}
The plus or minus 1 on the exponent turns into either multiplication or division by 1.38. 

The posterior intervals are tighter for the lognormal model, which
generally indicates a better fit to data.  Let's see what the data and
some posterior draws of the regression look like on the log scale.

```{python}
df = iris_data_frame()
df['log_petal_length'] = np.log(df['petal_length'])
df['log_petal_width'] = np.log(df['petal_width'])
alpha_draws = log_sample.stan_variable('alpha')
beta_draws = log_sample.stan_variable('beta')
plot =  pn.ggplot(df, pn.aes(x='log_petal_width',
                             y='log_petal_length',
			     color='species'))
plot = plot +  pn.geom_point()
plot = plot +  pn.labs(x = "petal width (log cm)",
                       y = "petal length (log cm)")
for a, b in zip(alpha_draws[0:20], beta_draws[0:20]):
    plot = plot + pn.geom_abline(intercept = a, slope = b,
                                 alpha = 0.5, size = 0.2)
mydraw(
  plot
)
```

Now the *iris versicolor* and *iris virginica* look good, but the
smaller *iris setosa* is still poorly characterized.  We'll fix this
problem in the next section.

## Multi-indexing: varying slopes and varying intercepts

We can see from the previous section that a single regression line
doesn't fit all three species of iris well.  While the lognormal model
is better, it still fails to capture *iris setosa* well.  On the log
scale, it does look like it will be possible to capture *iris
setosa*'s scale, as long we can build separate regressions for each.
The varying slopes and varying intercepts are both types of varying
effects which are sometimes called _random effects_, in contrast to
the previous models' _fixed effects_, which do not vary by group.

Mathematically, we now have three intercepts (and slopes), one for
each species of iris.  We will represent these as 3-vectors, $\alpha,
\beta \in \mathbb{R}^3$.  Given our $N$ data items, we will introduce
a series of indexes $\textrm{species} \in \{ 1, 2, 3 \}^N$, where
$\textrm{species}_n$ is the species of the $n$-th data item.  Sticking
to the log scale, our regression now looks like this,
$$
\textrm{length}_n
\sim \textrm{lognormal}(\alpha_{\textrm{species}_n}
                         + \beta_{\textrm{species}_n} \cdot \log \textrm{width}_n,
                        \sigma).
$$
The value $\alpha_{\textrm{species}_n}$ is the intercept for
$\textrm{species}_n \in \{1, 2, 3\}$.  We have kept the same
error term $\sigma$, though we could have split that out on
a per-species basis like the slope and intercept, too.

The Stan code follows the statistical notation.  

```{.stan filename='../stan/iris-petals-varying.stan'}
data {
  int<lower=0> N;
  vector<lower=0>[N] petal_width;
  vector<lower=0>[N] petal_length;
  int<lower=0> K;
  array[N] int<lower=1, upper=K> species;
}
transformed data {
  vector[N] log_petal_width = log(petal_width);
}
parameters {
  vector[K] alpha;
  vector[K] beta;
  real<lower=0> sigma;
}
model {
  petal_length ~ lognormal(alpha[species]
                             + beta[species] .* log_petal_width,
                           sigma);
  alpha ~ normal(0, 3);
  beta ~ normal(0, 3);
  sigma ~ lognormal(0, 0.6);
}
```

Here, we take $K$ to be the number of species and the species
indicator thus takes on values between $1$ and $K$ (inclusive).  Where
before we had `alpha` and `beta` for the intercept and slope, we now
have `alpha[species]` and `beta[species]`.  Further, the product of
the slope and width is now an elementwise product (`.*`).  The
sampling statement above is equivalent to
```{stan}
for (n in 1:N) {
  petal_length[n]
    ~ lognormal(alpha[species[n]]
                  + beta[species[n]] * log_petal_width[n],
                sigma);
}
```

### Elementwise products in Stan

The elementwise product (`.*`) uses MATLAB syntax and is defined so that
```stan
(a .* b)[n] == a[n] * b[n]  // true
```
Elementwise division (`./`) works the same way and we don't need
elementwise addition or subtraction because those operations are already
defined as regular vector addition and subtraction.

### Multi-indexing in Stan

This model uses a technique we call multi-indexing and it works the
similarly to the way indexing works in R.  Suppose we have a vector
of size `J` and an array of indexes of size `N`.  
```stan
array[N] int idxs;
vector[J] foo;
```
We can use the multiple indexes in `idxs` to index `foo` as `foo[idxs]`.
the result is a size `N` vector (the type is taken from `foo` and the
size from `idxs`), with values given by indexing into `idxs`, e.g., for
`n` in `1:N`, we have
```stan
cols(foo[idxs]) == N          // true
foo[idxs][n] == foo[idxs[n]]  // true
```
which tells the number of columns of `foo[idxs]` is `N` and
that indexing is done by indexing into `idxs`.

This works the same way for range indexing, such as `foo[1:3]`,
and for indexing with array literals like `{7, 9, 3}`, e.g.,
```{stan}
foo[3:5] == {foo[3], foo[4], foo[5]}         // true
foo[{7, 9, 3}] == {foo[7], foo[9], foo[3]}   // true
```

### Fitting our varying effects model

First, we add a `species` column to our data frame to represent the species
of the iris as an integer 1, 2, or 3.  Then we will just read in
the data and fit and summarize the results.

```{python}
vary_model = csp.CmdStanModel(
                  stan_file = '../stan/iris-petals-varying.stan')
vary_sample = vary_model.sample(data = iris_data(), seed = 123,
                                show_progress = False,
				show_console = False)
vary_sample.summary(sig_figs = 2)
```

There are a few things worth noting in this summary. First, recall
that the parameter estimates in the fixed effects model were
$\widehat{\alpha} = 1.3$, $\widehat{\beta} = 0.57$, and
$\widehat{\sigma} = 0.16$. In the varying effects model, we see that
the estimates for $\alpha_k$ and $\beta_k$ vary considerably by $k$.
For example, as we see in the data, there is very little effect of
width on length for *iris setosa*, with $\widehat{\beta}_1 = 0.075$,
wheres there is a large effect for *iris * of $\widehat{\beta}_2 =
0.6$. Furthermore, our value for $\sigma$ is lower, which indicates
lower residual error in our predictors. We will leave evaluating
varying error scales to the reader.


We now have three different regression lines, which we plot on the
log scale and color to match the data.

```{python}
df = iris_data_frame()
df['log_petal_length'] = np.log(df['petal_length'])
df['log_petal_width'] = np.log(df['petal_width'])
plot =  pn.ggplot(df, pn.aes(x='log_petal_width',
                             y='log_petal_length',
			     color='species'))
plot = plot +  pn.geom_point()
plot = plot +  pn.labs(x = "petal width (log cm)",
                       y = "petal length (log cm)")
plot = plot + pn.geom_abline(intercept = .49, slope = 0.076, color='red')
plot = plot + pn.geom_abline(intercept = 1.30, slope = 0.59, color='green')
plot = plot + pn.geom_abline(intercept = 1.50, slope = 0.23, color='blue')
mydraw(
  plot
)
```

# Containers: arrays, vectors, and matrices

Stan is strongly typed, and each of the types has a distinct purpose.
Constraints act as error checks in the data, transformed data,
transformed parameter, and generated quantities blocks.  In the
parameter block, they are used to transform from unconstrained to
constrained values (with implicit change of variables adjustment).
The full set of transforms and their log absolute Jacobian
determinants are given in the _Stan Reference Manual_.

## Integer and real primitives

The two primitive types are `int` and `real`.  In the compiled C++
code, these are 32-bit signed integers and 64-bit floating point
values, respectively.

The third scalar type is `complex`, where a complex value consists of
a real component and an imaginary component, both of which are real
numbers.

### Constrained primitive types

Integer and real types may be constrained with lower bounds, upper
bounds, or both.  For example, `real<lower=0>` is the type for a
concentration, `real<lower=0, upper=1>` is the type for a probability,
`real<lower=-1, lower=1> is the type for a correlation, 
`int<lower=0>` is the type for a count, and `int<lower=1, upper=5>` is
the type for an ordinal response to a survey.

## Arrays

For any type `T`, we can write `array[N] T` for the type of an array
of size `N` containing elements of type `T`.  We can also write
`array[M, N]` for a 2D array and so on for higher dimensionality.

## Matrices

The type `matrix[M, N]` is for an $M \times N$ matrix.  The type
`complex_matrix[M, N]` is for an $M \times N$ matrix with complex values.
We can assign real matrices to complex matrices, but not vice-versa.


### Constrained matrix types

There are four special matrix types for positive definite and unit
positive definite matrices and their Cholesky factors, `cov_matrix`,
`cholesky_factor_cov`, `corr_matrix`, and `cholesky_factor_corr`.  The
Cholesky factors are lower triangular.


## Vectors and row vectors

The type `vector[M]` is for column vectors with `M` rows, whereas
`row_vector[N]` is for row vectors with `N` columns.  A column vector
is like an $M \times 1$ matrix and a row vector is like a $1 \times N$
vector.  There are also `complex_vector` and `complex_row_vector`
types which work the same way.

### Constrained vector types

There are constrained vector types for (upward) ordered vectors
(`ordered[M]`), positive (upward) ordered vectors (`pos_ordered[M]`),
unit vectors where the sum of squared values is one
(`unit_vector[M]`), and simplexes where the sum of the non-negative
values is 1 (`simplex[M]`).  All of these produce size `M` vectors
matching the constraints.


# Discrete parameters

Although Stan only allows parameters which are real or complex valued,
it is able to work with models involving discrete parameters by
marginalizing them out in the likelihood and working in expectation
during inference.

There are two reasons Stan doesn't allow discrete parameters.  First,
because Stan isn't limited to directed graphical models, it's
technically impossible to pull out efficient conditional distributions
for the discrete parameters in general (though it can be done in some
limited cases).  Second, discrete sampling tends not to perform very
well because the parameters tend to get locked and not mix well.


## Normal mixture model

We'll consider a simple two-component normal mixture model
parameterized with a discrete responsibility parameter.  This model
assumes that each item $y_n$ is drawn from one of two possible normal
distributions, $\textrm{normal}(\mu_1, \sigma_1)$ and
$\textrm{normal}(\mu_2, \sigma_2)$, with a probability $\lambda \in
[0, 1]$ of being drawn from the frist distribution.  Let's assume we
have $N$ observations $y_n \in \mathbb{R}$.  We will use $z_n \in \{
0, 1 \}$ as the discrete parameter representing the distribution from
which $y_n$ arose.  This gives us the following model
\begin{align}
z_n &\sim \textrm{bernoulli}(\lambda)
\\[6pt]
y_n &\sim \textrm{normal}(\mu_{z_n}, \sigma_{z_n})
\end{align}
In frequentist settings, the parameters $z_n$ are sometimes called
_missing data_ to finesse a philosophical aversion to probability
distribution over parameters.

## Marginalization to the rescue

The immediate problem is that we cannot code this directly as written
in Stan because we do not have discrete parameters.  But what we can
do is _marginalize_ out the discrete parameters.  We know from the
_law of total probability_ that if $B$ is a discrete random variable,
then we can derive the marginal probability $p(a)$ from the joint
probability function $p(a, b)$ by
$$
p(a) = \sum_{b \in B} p(a, b).
$$

We can put our likelihood in this form as
$$
p(y, z \mid \lambda, \mu, \sigma)
= \prod_{m = 1}^M p(y_n, z_n \mid \lambda, \mu, \sigma).
$$
Then we can evaluate the likelihood elementwise,
$$
p(y_n, z_n \mid \lambda, \mu, \sigma)
= \textrm{bernoulli}(z_n \mid \lambda)
  \cdot \textrm{normal}(y_n \mid \mu_{z_n}, \sigma_{z_n}).
$$
Now we can marginalize out the $z_n$ by considering values 0 and 1,
\begin{align}
p(y_n \mid \lambda, \mu, \sigma)
&= p(y_n, z_n = 0 \mid \lambda, \mu, \sigma)
  + p(y_n, z_n = 1 \mid \lambda, \mu, \sigma)
\\[6pt]
&= \textrm{bernoulli}(0 \mid \lambda) \cdot \textrm{normal}(y_n \mid \mu_0, \sigma_0)
\\[2pt]
& \qquad + \ \textrm{bernoulli}(1 \mid \lambda) \cdot \textrm{normal}(y_n \mid \mu_1, \sigma_1).
\end{align}
We can further substitute in the value for the Bernoulli densities,
which are
\begin{align}
\textrm{bernoulli}(0 \mid \lambda) &= 1 - \lambda, \ \textrm{and}
\\[4pt]
\textrm{bernoulli}(1 \mid \lambda) &= \lambda,
\end{align}
to produce
$$
p(y_n \mid \lambda, \mu, \sigma)
= (1 - \lambda) \cdot \textrm{normal}(y_n \mid \mu_0, \sigma_0)
+ \lambda \cdot \textrm{normal}(y_n \mid \mu_1, \sigma_1).
$$




## Simulated data: adult height of men and women

Let's consider a specific instance, a collection of height data where
men and women have not been discriminated.  Let's assume for the sake
of simulation that men have a mean height of 175cm with standard
deviation of 7.5cm and women have a mean height of 162cm with a
standard deviation of 6.3cm.  We'll further assume that women make up
51% of the adult population (this varies dramatically across ages).
This will allow us to simulate data for 5000 randomly selected
heights.

```{python}
M = 50 # 5_000
female_mean_height_cm = 162
male_mean_height_cm = 175
female_sd_height_cm = 6.3
male_sd_height_cm = 7.5
prob_female = 0.51
mu = np.array([female_mean_height_cm, male_mean_height_cm])
sigma = np.array([female_sd_height_cm, male_sd_height_cm])
sex = np.array(np.random.binomial(n = 1, p = prob_female, size=M))
heights = np.random.normal(loc = mu[sex], scale = sigma[sex], size=M)
```


## The log scale and log sum of exponents operation

We have successfully marginalized out the $z$ and derived $p(y \mid
\lambda, \mu, \sigma)$, but this leaves us with a residual
problem---Stan works on the log scale and we have done our derivation
on the linear scale.  Well, all we need to do is take the log and
we're done.
\begin{align}
\log p(y_n \mid \lambda, \mu, \sigma)
=
\log\! \big( \, \strut & \textrm{bernoulli}(0 \mid \lambda) \cdot \textrm{normal}(y_n \mid \mu_0, \sigma_0)
\\[2pt]
& +  \textrm{bernoulli}(1 \mid \lambda) \cdot \textrm{normal}(y_n \mid
\mu_1, \sigma_1)\, \big).
\end{align}
To reduce this further, we are going to use the _log-sum-exp_ operation,
which is defined by
$$
\textrm{log-sum-exp}(a, b)
= \log \left( \strut \exp(a) + \exp(b)\right).
$$

The reason we use log-sum-exp is that it can be made arithmetically
stable in such a way as to prevent numerical overflow by noting
$$
\textrm{log-sum-exp}(a, b)
= \textrm{max}(a, b)
  + \textrm{log-sum-exp}(a - \textrm{max}(a, b),
                         b - \textrm{max}(a, b))
\\[4pt]
$$
This form eliminates numerical overflow because exponentiation is only
applied to the terms $\left( a - \textrm{max}(a, b)\right)$ and
$\left(b - \textrm{max}(a, b)\right)$, both of which must be less than
or equal to zero by construction.  Underflow is mitigated by pulling
the leading term $\textrm{max}(a, b)$ out of the sum.

Using log-sum-exp, we can now rewrite our marginalized individual item
likelihood as
\begin{align}
\log p(y_n \mid \lambda, \mu, \sigma)
= \textrm{log-sum-exp}\big( &\log \textrm{bernoulli}(0 \mid \lambda)
+ \log \textrm{normal}(y_n \mid \mu_0, \sigma_0),
\\[2pt]
&\log \textrm{bernoulli}(1 \mid \lambda)
+ \log \textrm{normal}(y_n \mid \mu_1, \sigma_1)
\big).
\end{align}

## Stan program for mixtures

As in our earlier examples, the Stan code for mixtures just follows
the mathematical definitions.

```{.stan filename="heights.stan"}
data {
  int<lower=0> M;
  vector<lower=0>[M] heights;
}
parameters {
  real<lower=0, upper=1> lambda;
  ordered[2] mu;
  vector<lower=0>[2] sigma;
}
model {
  mu ~ normal(170, 10);
  sigma ~ lognormal(log(7), 1);
  for (m in 1:M) {
    real lp1 = bernoulli_lpmf(0 | lambda)
        + normal_lpdf(heights[m] | mu[1], sigma[1]);
    real lp2 = bernoulli_lpmf(1 | lambda)
        + normal_lpdf(heights[m] | mu[2], sigma[2]);
    target += log_sum_exp(lp1, lp2);
  }
}
```
This program uses local variables `lp1` and `lp2`.  These have scope
just within the for-loop and take on different values with each loop
iteration.  In general, local variables can be assigned and reassigned
in Stan.  They are declared with sizes, but cannot have constraints.

Unlike in the original model, we have to index `mu` and `sigma` from 1
because Stan arrays are indexed from 1.  The biggest departure from
the mathematical model as written is that we have declared `mu` to be
of type `ordered`, which is the type of a vector of increasing values.
The reason we do this is that the indexes are not well
identified---swapping them preserves the likelihood.

We have included weakly informative priors for `mu` and `sigma` based
on our prior knowledge of heights.  The likelihood then just follows
the mathematical definition.  We had to resort to a loop because Stan
isn't able to vectorize mixtures.

## Fitting the simulated data

Now we can fit our simulated data.

```{python}
model = csp.CmdStanModel(stan_file = '../stan/heights.stan')
data = {'M': M, 'heights': heights}
sample = model.sample(data = data, seed = 123,
                      show_progress = False, show_console = False)
sample.summary(sig_figs = 2)	      
```

Mixture models are quite challenging to fit when the components have
similar distributions as the normals with close locations and scales
do here.  Checking the fit, we find $\widehat{R}$ values as high as
1.01 (still acceptable) and effective sample sizes of around 1/6 the
number of iterations.  Nevertheless, we see the Monte Carlo standard
error is relatively low for all posterior means other than for
$\lambda$.  Although we recover reasonable estimates for
all parameters (simulated values were $\lambda = 0.51$, $\mu = [162 \ 175]$
and $\sigma = [6.3 \ 7.5]$), the posterior intervals are only well constrained for
`mu`, with `sigma` being much wider and `lambda` being very poorly
identified.  These intervals grow with less data and shrink with more
data.


## Built-in Stan function for mixtures


For simple mixtures of this kind, Stan supplies a built-in function
```
log_mix(lambda, lp1, lp2)
    = log_sum_exp(log(lambda) + lp1,
                  log1m(lambda) + lp2)
```
which is a more efficient drop-in replacement for the explicit
definition in our example code, the body of the loop in the model of
which can just be
```stan
target += log_mix(lambda, normal_lpdf(heights[m] | mu[1], sigma[1]),
                          normal_lpdf(heights[m] | mu[2], sigma[2]));
```
This may run a bit faster, but it won't mix any better as it defines
exactly the same target log density as the long form.

## Recovering posterior distributions over discrete parameters

We have marginalized out the discrete parameters, but what if they are
of interest in the fitted model?  It turns out, we can sample $z$ in
the generated quantities block.  All we have to do is work out the
probability that $z_n = 0$ (female) and sample.  Once we have this
probability, we can calculate many quantities of interest in
expectation rather than working with the sample (i.e., we will
_Rao-Blackwellize_).  The calculation for the expected sex given
values of $\mu$ and $\sigma$ and $y_n$ is
$$
\textrm{Pr}[Z_n = 1 \mid y]
\ = \
\mathbb{E}[Z_n \mid y]
\ = \
\frac{\displastyle p(y_n \mid \mu_0, \sigma_0)}
       {\displaystyle p(y_n \mid \mu_0, \sigma_0)
                      + p(y_n \mid \mu_1, \sigma_{z_n})}
$$
On the log scale where Stan operates, that's
$$
\log \textrm{Pr}[Z_n = 1 \mid y]
= \log p(y_n \mid \mu_0, \sigma_0)
  - \textrm{log-sum-exp}(\log p(y_n \mid \mu_0, \sigma_0),
                         \log p(y_n \mid \mu_1, \sigma_1).
$$


Here's a new generated quantities block we can add to the last
program, `heights.stan`, in order to generate the probability that the
first 10 data items are heights for women, and to randomly generate
the sex for the first ten individuals based on this probability.

```{.stan filename='../stan/heights-post.stan'}
generated quantities {
  array[10] int<lower=0, upper=1> sex;
  vector<lower=0, upper=1>[10] Pr_female;
  for (m in 1:10) {
    real lp1 = bernoulli_lpmf(0 | lambda)
               + normal_lpdf(heights[m] | mu[1], sigma[1]);
    real lp2 = bernoulli_lpmf(1 | lambda)
               + normal_lpdf(heights[m] | mu[2], sigma[2]);
    Pr_female[m] = exp(lp1 - log_sum_exp(lp1, lp2));
  }
  array[10] int<lower=0, upper=1> sex = bernoulli_rng(Pr_female);
}
```

Now we can compile and fit this model.

```{python}
model = csp.CmdStanModel(stan_file = '../stan/heights-post.stan')
data = {'M': M, 'heights': heights}
sample = model.sample(data = data, seed = 123,
                      show_progress = False, show_console = False)
sample.summary(sig_figs = 2)	      
```

The probabilities range from 0 to 1 with some intermediate values.
The means after sampling are close to the means of the direct
probability estimate, but much noisier (i.e., higher MCMC standard
error).  We can compare the probabilities to the simulated values of
$z$, 
```{python}
print(f"{sex[0:10] =}")
```

# Debugging Stan programs

As usual, we have presented a series of Stan programs that actually
work as intended.  When developing new programs, one often runs into
problems.

## Failure to provide support over constrained parameters

Stan requires programs to assign a finite log density to any
parameters that satisfy the constraints.  For example, the following
program puts no constraints on `sigma` in the declaration, but the
model block requires `sigma` to be greater than zero.
```stan
parameters {
  real sigma;
}
model {
  sigma ~ lognormal(0, 1);
}  
```
This violates Stan's requirements because the declared constraints
allow a negative value for `sigma`, but the model block does not.  The
way to fix this is to declare `sigma` to be of type `real<lower=0>`.
Often this kind of bug shows up as a failure to randomly initialize.
For instance, if we had `vector[10] sigma`, we'd only have a $2^{-10}$
or about 1/1000 chance of generating valid initializations at random,
because Stan initializes parameters using a $\textrm{uniform}(-2, 2)$
distribution on the unconstrained scale.

## Debug by print

Stan has a built-in `print()` function that can take string and Stan
variable arguments, which will be printed when it is executed.  In
particular, printing the `target()` value can help detect when it
becomes $-\infty$ (and thus causes rejections).  This can be done line
by line, for example,
```stan
model {
  y ~ normal(alpha + beta * x, sigma);
  print("step 17, target() = ", target());
  sigma ~ lognormal(0, 1);
  print("step 18. target() = ", target());
}
```
Then if one of the statements throws an exception, you will see the
print statement just before it was called. If one of the statements
leads to a not-a-number or negative infinity value, you will see the
first place this affects `target()`. You can also print the results of
intermediate calculations.  Be careful printing long structures; you
might want to try printing a slice, e.g.,
```stan
matrix[256, 256] a = b * c;
print("a[1:3, 5:6] = ", a[1, 1]);
```



# Making Stan programs go faster

For MCMC, there are two independent components to efficiency, (1) the
_statistical efficiency_ of the sampler, which is measured by
effective sample size per iteration, and (2) _program efficiency_,
which is measured by how long the program takes to compute the log
density and gradients.

## Computational efficiency

There are many ways to write a Stan program to compute the log density
and gradients of a model with respect to a fixed parameter vector.
Some of them are faster than others.  For speeding up Stan programs,
most of the usual considerations apply.  Specifically, we want to
optimize memory locality and we want to reduce branch points; together
these are the main efficiency bottlenecks for modern code.

### Working first, then optimize

As Donald Knuth is reputed to have said, "Premature optimization is
the root of all evil."  It's pretty much always a good idea to get a
simple version of your code working on a small slice of data first.
Then only worry about optimization if you need it and if you do need
it, don't try it without an unoptimized form that works as backup.
Every developer has learned through hard experience that it's often a
whole lot easier to optimize working code than it is to debug
optimized code.

### Do not repeat yourself

The main rule for making Stan programs go fast is *don't repeat
yourself*.  If there is a computation, don't do that computation
again, save the result in a local variable and reuse.  For example,
don't do this:
```stan
for (n in 1:N) {
  y[n] ~ normal(alpha + beta * x[n], exp(log_sigma));
}
```

The problem here is that `exp(log_sigma)` gets computed `N` times.
This uses more memory, as Stan records every operation with links to
its operands during.  It is also slow, with `N - 1` redundant
applications of the chain rule (a multiply/add).  Instead, the code should be refactored to

```stan
real sigma = exp(log_sigma);
for (n in 1:N) {
  y[n] ~ normal(alpha + beta * x[n], sigma);
}
```

### Avoid autodiff if possible

If you have a constant that's being defined, define it in the
transformed data block.  That way it only gets evaluated once.  For example, rather than doing this,
```stan
data {
  real<lower=0> alpha;
}
model {
  theta ~ dirichlet(rep_vector(alpha, K));
}  
```
then it's much better to do this,
```stan
transformed data {
  vector<lower=0>[K] alphas = rep_vector(alpha, K);
}
model {
  theta ~ dirichlet(alphas);
}
```

The other case to do this is for generated quantities. If it's
possible to move a variable to the generated quantities block, it
should be moved.  For example, we might be tempted do posterior
predictive simulation as follows.
```stan
parameters {
  real alpha, beta;
  real<lower=0> sigma;
  vector[M_tilde] y_tilde;
}
model {
  y ~ normal(alpha + beta * x, sigma);
  y_tilde ~ normal(alpha + beta * x_tilde, sigma);
}
```
In situations like this, we can move `y_tilde` down to generated
quantities because it's conditionally independent given `mu` and `sigma`.
```stan
parameters {
  real alpha, beta;
  real<lower=0> sigma;
}
model {
  y ~ normal(alpha + beta * x, sigma);
}
generated quantities {
  array[M_tilde] y_tilde = normal_rng(alpha + beta * x_tilde, sigma);
}  
```
If we really need a vector for `y_tilde` for some reason, we can
convert with `to_vector()`.


### Do not use diagonal multivariate normal

Instead of
```stan
y ~ multi_normal(mu, diag_matrix(sigma));
```
which creates a matrix then factors to compute its inverse and determinant, we can use the univariate normal,
```stan
y ~ normal(mu, sigma);
```
The result is the same up to a constant.


### Memory locality, copying, and storage order

Accessing a row or column of a matrix requires allocating memory and
copying.  Memory pressure is one of the biggest constraints in modern
code and relieving it is one of the best things to do to make code go
faster.

Matrices are organized in column-major order in memory, whereas arrays
are row major.  That means that the columns of a matrix are stored
together in memory, and therefore `Sigma[1:M, n]` is efficient, but
`Sigma[m, 1:N]` is not if `M` is large.

If you need to access vectors one by one but you do not need matrix
operations, use an array of vectors in either order:
```stan
matrix[M, N] alpha;
array[M] row_vector[N] alpha_arr;
array[N] vector[M] alpha_t_arr;  // transposed
```
Accessing elements of an array does not require any copying.

Overall, copying and rearranging data structures is not very expensive
in Stan.  Loops are very fast, and rearranging doesn't add any
overhead to the automatic differentiation.


### Vectorize non-linear operations over parameters

Even though loops are very fast in Stan, every expression involving
parameters leads to nodes and edges being added to the expression
graph.  The usual suspect is sampling in a loop, as follows.
```stan
for (n in 1:N) {
  y[n] ~ normal(alpha + beta * x[n], sigma);
}
```
Written this way, Stan isn't smart enough to realize it can compute
`log(sigma)` once and re-use.  It will also build a huge number of
edges from the log density to `alpha` and `beta`, one for each `y[n]`.  On the other hand, it's much faster written as follows.
```stan
y ~ normal(alpha + beta * x, sigma);
```

The speed from vectorization is not because loops are slow, it's
because doing autodiff in a loop leads to a much larger expression
graph and hence much more work when updating the chain rule.  The
vectorized form has a single output variable for the log density with
three edges leading to `alpha`, `beta`, and `sigma`.  It will also be
clever enough to calculate `log(sigma)` once and reuse it.  This
advantage is even bigger for distributions like multivariate normal
that require expensive internal determinant calculations.



## Statistical efficiency

By far the best solution for making Stan programs go faster is to
improve the statistical efficiency of the model.

### Identifiability

The first thing to consider is _identifiability_.  A likelihood $p(y
\mid \theta)$ is _identifiable_ if distinct parameters lead to
distinct likelihoods, or in symbols, if $\theta \neq \theta'$ implies
$p(y \mid \theta) \neq p(y \mid \theta')$.  The other way around, a
likelihood is not identifiable if there are $\theta \neq \theta'$ such
that $p(y \mid \theta) = p(y \mid \theta')$.

The classic _Bradley-Terry model_ for inferring quality based on
pairwise comparisons has a non-identifiable likelihood.  Let's
consider an application to sports team ranking (it was originally
applied to consumer products).  Each team $j$ has an ability ranking
$\alpha_j$.  The log odds of team $j$ defeating team $k$ is modeled as
$\alpha_j - \alpha_k$.  The observations are of the results of games
$n$ between team $A_n$ and $B_n$ (the notation is meant to be
suggestive of the original application to A/B testing).  This defines
the likelihood
$$
p(y_n \mid \alpha) = \textrm{bernoulli}(\textrm{logit}^{-1}(\alpha_{A_n} - \alpha_{B_n}).
$$
In Stan, this likelihood can be coded as
```stan
model {
  y ~ bernoulli_logit(alpha[A] - alpha[B]);
}
```
where the vectorized sampling statement unfolds to the following
equivalent, but less efficient, form.
```stan
  for (n in 1:N) {
    y[n] ~ bernoulli_logit(alpha[A[n]] - alpha[B[n]]);
  }    
```
The model is clearly non-identifiable because
$$
p(y_n \mid \alpha) = p(y_n \mid \alpha + c).
$$

To get around this problem, we can do three things.  The first two
involve reducing the number of free parameters and the third involves
soft identification through a prior.  First, we can pin one of the
values $\alpha$, traditionally by setting $\alpha_1 = 0$. 
This reduces the number of free parameters by one, as is reflected in
the Stan parameterization.  
```stan
parameters {
  vector[J - 1] alpha_rest;
}
transformed parameters {
  vector[J] alpha = append_row(0, alpha_rest);
}
```
We can then work with `alpha` directly.  In the second approach, we
can enforce the constraint $\textrm{sum}(\alpha) = 0$.  This also
reduces the number of free parameters by one and is coded in a similar
way, only with a different transform.
```stan
transformed parameters {
  vector[J] alpha = append_row(-sum(alpha_rest), alpha_rest);
}  
```
On the plus side, this approach is symmetric. On the downside, it is
not generative in that it is not clear how to add a new player $J +
1$.

The third way to make a Bayesian model identifiable is to add a prior.
For example, we can do this,
```stan
model {
  alpha ~ normal(0, 3);
}
```
This provides a kind of _soft identification_ in the posterior, where
values of `alpha` near 0 have higher posterior density.  The third
approach is overparameterized but has a pleasing symmetry and
generalizability to new players.


### Adaptation and preconditioning

The _condition number_ of a positive-definite matrix is the ratio of
its largest eigenvalue to its smallest.  For a density, we can
consider the negative inverse Hessian of its log density at a point.
For a multivariate normal distribution, the negative inverse Hessian
is its covariance at every point.  That is, a normal distribution has
_constant curvature_.  

The larger the condition number, the harder it is to sample from a
model, because multiple steps at the scale of the smallest eigenvalue
are required to take a step in the direction of the largest
eigenvalue.  The first-order gradient-based approximation that
Hamiltonian Monte Carlo uses can also fail due to high curvature; when
the step is too large, the result will be _divergent_ Hamiltonian
trajectories (i.e., ones that do not preserve the Hamiltonian).

When the inverse Hessian is not positive definite, the density is not
convex, which can present an even greater challenge to sampling than
poor conditioning. If the inverse Hessian is positive definite and
relatively constant over the posterior, we can correct for non-unit
conditioning by _preconditioning_, which can rotate and scale a
density until it looks more like a standard normal.  In fact, given
a multivariate normal it does just that---rotates and scales back to a
standard normal.

Stan begins adaptation in a state that assumes the posterior is
standard normal, which is (a) centered at the origin, (b) independent
in dimension, and (c) unit scale.  The closer a posterior is to
standard normal, the easier it will be for Stan to adapt and to
sample.

During adaptation, Stan estimates the posterior (co)variance to use
for preconditioning.  If the posterior Hessian is constant, the
covariance will be equal to the negative inverse Hessian everywhere.

With its default settings, Stan preconditions by scaling each of the
parameters as close to unit scale as it can manage.  Under this
scheme, the adapted metric requires $\mathcal{O}(D)$ storage and
each leapfrog step (the most inner loop in Stan) requires
$\mathcal{O}(D)$ time.  By selecting a dense metric, Stan will also
adapt to a fixed curvature (i.e., one that does not vary over the
posterior). This requires $\mathcal{O}(D^2)$ storage, a one-time
$\mathcal{O}(D^3)$ cost to Cholesky factor, and then requires
$\mathcal{O}(D^2)$ time per leapfrog step to evaluate.

### Reparameterization

We can often reparameterize models to make them easier to sample.
The standard example is reparameterizing a varying effect to use a
_non-centered parameterization_.  The centered parameterization is
```stan
parameters {
  real<lower=0> sigma;
  vector[N] alpha;
}
model {
  sigma ~ lognormal(0, 1.5);
  alpha ~ normal(0, sigma);
}
```
This model induces a strong dependence between `sigma` and `alpha`.
When `sigma` is small`, `alpha` must be small, and when `sigma` is
large, `alpha` will vary over a wide range.  This varying curvature
(high curvature for low `sigma`, very flat expanses for high `sigma`)
is very challenging for MCMC sampling.

We can overcome the difficulty in this parameterization by
non-centering, which we can code explicitly as follows.
```stan
parameters {
  real<lower=0> sigma;
  vector[N] alpha_std;
}
transformed parameters {
  alpha = sigma * alpha_std;
}
model {
  sigma ~ lognormal(0, 1.5);
  alpha_std ~ normal(0, 1);
}
```
We now have an independent standard normal distribution on
`alpha_std`, not on `alpha`.  It induces the correct distribution on
the transformed parameter `alpha`, so that `alpha` has a `normal(0,
sigma)` distribution.  While this correction is only in the prior,
if there is not a lot of data for each `n`, the posterior will be similar.

### Priors for speed and knowledge

Often we are tempted to use vague or overdispersed priors.  When this
gets too extreme, it can cause computational difficulties navigating
flat expanses of probability space.  If you have used vague priors to
avoid thinking too hard and computation is going awry, you might
consider adding priors with a bit more information.  It's often
possible to impose _weakly informative priors_ that only determine the
scale of a variable, which is often known.  Priors are particularly
important when there is not much data.  Also, it's nice to actually
use real prior knowledge in inference because not doing so is just
leaving useful information on the table.


# Appendices

## Bayesian statistics

@bayes1763 introduced the paradigm of statistical inference that has
come to be known as Bayesian statistics.  In retrospect, Bayes's idea
is quite simple, involving four conceptually simple steps.

1.  Define a parametric _sampling density_ $p(y \mid \theta)$ which
describes how to generate observations $y$ given parameters $\theta$.
2.  Define a _prior density_ $p(\theta)$ capturing what is known before
observing new data.
3.  Derive the _posterior density_ $p(\theta \mid y) \propto p(y \mid
\theta) \cdot p(\theta)$ via Bayes's rule.
4.  Evaluate _event probabilities_ conditioned on observed data $y$,
such as $\textrm{Pr}[\textrm{cond}(\theta) \mid y]$ for some condition
on the parameters by integrating over the posterior. 

We work through an example end-to-end in the next section.

### Bayes's theorem

Bayes
formalized his approach in the following theorem.

::: {#thm-line}
### Bayes's Theorem

Given a joint density $p(y, \theta)$, the posterior density $p(\theta
\mid y)$ can be defined in terms that only involve the prior
$p(\theta)$ and sampling distribution $p(y \mid \theta)$, as
$$
p(\theta \mid y)
\ = \
\frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y \mid \theta) \cdot p(\theta) \,
        \textrm{d}\theta}.
$$

_Proof_:
\begin{align}
p(\theta \mid y)
&=  \frac{p(y, \theta)}
         {p(y)}
& \textrm{[definition of conditional probability]}       
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {p(y)}
& \textrm{[chain rule]} 
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y, \theta) \, \textrm{d}\theta}
& \textrm{[law of total probability]}
\\[6pt]
&= \frac{p(y \mid \theta) \cdot p(\theta)}
        {\int_{\Theta} \, p(y \mid \theta) \cdot p(\theta) \,
        \textrm{d}\theta}.
& \textrm{[chain rule]} 
\end{align}
$\blacksquare$
:::

Bayes's theorem allows us to solve the so-called _inverse problem_ of
inferring the posterior $p(\theta \mid y)$ when all we have is the
sampling distribution $p(y \mid \theta)$, the prior $p(\theta)$, and
some observed data $y$.

In most casses, Stan programs only require densities defined up to a
normalizing constant.   This allows us to go a step further and drop
the denominator $p(y)$, which does not depend on $\theta$.
$$
p(\theta \mid y)
\ \propto \
p(y \mid \theta) \cdot p(\theta).
$$

This lets us proceed with only an unnormalized sampling density $p(y
\mid \theta)$ and unnormalized prior $p(\theta)$.  For some
applications to model comparison we will need a normalizing sampling
distribution $p(y \mid \theta)$ in order to make and compare
probabilistic predictions.

## Expectation notation

If we have a (potentially multivariate) random variable $Z$, we write
$\mathbb{E}[Z]$ for its _expectation_, which is defined as its average
value.  This assumes we have a fixed measure to provide a density
$p_Z(z)$ for the random variable $Z$.  We compute the average value by
averaging over the possible values for $Z$ with weights given by its
density,

$$
\mathbb{E}[Z] = \int_{Z} z \cdot p_Z(z) \, \textrm{d}z.
$$

With Bayesian statistics, we are typically interested in conditional
expectations, which is the value of a random variable conditioned on
the observed value of a second random variable.  Suppose $Y$ is a
second random variable and we observe that $Y = y$.  The _conditional
expectation_ of $Z$ given $Y = y$ is written
$$
\mathbb{E}[Z \mid Y = y]
= \int_{Z} z \cdot p(z \mid y) \, \textrm{d}z.
$$
That is, we take a weighted average of the value of $Z$ with weights
determined by the posterior density $p_{Z \mid Y}(z \mid y)$.  As
before, this assumes a fixed, joint measure for $Y$ and $Z$.

We are typically concerned with cases where $Z = f(\Theta)$ is a
function of the parameters $\Theta$ (and perhaps some data, as well),
and the conditioning is with respect to observed data $y$, where we
will drop the explicit $Y = y$ conditioning and just use $y$,
$$
\mathbb{E}[f(\Theta) \mid y]
= \int_{\Theta} f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta.
$$
These posterior expectations simply average the value of the function
$f(\theta)$ over the posterior $p(\theta \mid y)$.  All of our
inferences proceed this way---by averaging a quantity of interest
expressed as a function of parameters over the posterior



## Quantiles, medians, and uncertainty intervals

Suppose $Z$ is a univariate random variable (e.g., one of the
parameters, $\Theta_d$).  If $p \in [0, 1]$, the _$p$-quantile_ for a
univariate random variable $Z$, $\textrm{quantile}(Z, p)$, is defined
by
$$
\textrm{quantile}(Z, p) = z
\ \textrm{ if and only if } \
\textrm{Pr}[Z \leq z] = p.
$$
That is, the $p$-quantile of $Z$ is the value $z$ such that the
probability $Z$ is less than or equal to $z$ is $p$.  In Bayesian
applications, we will typically be taking the probability for
functions of parameters $Z = f(\Theta)$ and conditioning on observed data $y$.

The 0.5-quantile is known as the _median_.  There is a 50% chance that
a random variable takes on a value less than its median and a 50%
chance that it takes on a value greater.  Another common Bayesian
estimator for parameters $\Theta$ is their posterior median,
$$
\overline{\theta} = \textrm{quantile}(Z, 0.5).
$$
The median estimator has the property of minimizing expected absolute
<error (in contrast to the posterior mean estimate, which minimizes
expected squared error), again assuming the model represents the true
data generating process.


If we have two probabilities, $0 \leq p^L \leq p^U \leq 1$, they
define the $(p^L, p^U)$ _posterior interval_ 
$$
\textrm{interval}(Z, p^L, p^U)
= \left(
    \textrm{quantile}(Z, p^L),
    \textrm{quantile}(Z, p^U)
  \right).
$$

The probability that a point falls in the interval is given by the
differences in probabilities,
$$
\textrm{Pr}\!\left[Z \in \textrm{interval}(Z, p^L, p^U)\right] = p^U - p^L.
$$
If $1 - p^U = p^L$, the interval is called the _central $(p^L - p^U)$ interval_.
For example, the central 95% interval for $Z$ is
$\left( \textrm{quantile}(Z, 0.025), \textrm{quantile}(Z, 0.975) \right)$.


## Bayesian inference

Bayesian inference is largely about estimating quantities of interest
based on a probability model and observed data.  In typical applied
Bayesian inference problems, we are interested in three quantities
that can be expressed as expectations: parameter estimates, event
probabilities, and probabilistic prediction.  All of these quantities
are expressed as expectations over the posterior, meaning that they
involve averaging over our uncertainty in parameter values.

We are also interested in uncertainty, which is defined by the
posterior.  We typically summarize uncertainty using quantiles, and in
particular, posterior intervals (sometimes called "credible
intervals").


### Parameter estimation

The first quantity of interest is the value of parameters.  The
standard Bayesian parameter estimate is the posterior mean, or
conditional expectation given the data.  Given a model $p(y, \theta)$
and observed data $y$, the Bayesian posterior mean estimate of the
parameters $\theta$ is

\begin{align}
\widehat{\theta}
&= \mathbb{E}[\Theta \mid y]
\\[6pt]
&= \int_{\Theta} \theta \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

The posterior mean as a parameter estimate minimizes expected square
error in the estimates if the model is well specified in the sense of
representing the true data-generating process.  Squared error is the squared
$\textrm{L}_2$ norm of the difference between the estimate and the true
parameter values.  We can expand these definitions down to basic form.

\begin{align}
\widehat{\theta}
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. \left|\left| \, u - \Theta \, \right|\right|^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. (u - \Theta)^{\top} \cdot (u - \Theta)  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \mathbb{E}\!\left[\left. \sum_{d=1}^D \, (u_d - \Theta_d)^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \sum_{d=1}^D \, \mathbb{E}\!\left[ \left. (u_d - \Theta_d)^2  \, \right| \, y\right]
\\[6pt]
&= \textrm{arg min}_{u} \ \sum_{d=1}^D \, \int_{\Theta} (u_d - \theta_d)^2 \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

It's clear from the final form that the estimate $\widehat{\theta}$ is determined
by averaging over posterior uncertainty. 


### Event probabilities

An _event_ in statistics is a subset of the parameter space, $A
\subseteq \Theta$, where $\Theta$ is the set of all valid parameter
values.  We usually pick out events using conditions on the parameter
space.  For example, the condition $\theta > 0.5$ defines the
event $A = \left\{ \theta \in \Theta : \theta > 0.5 \right\}$.

The probability of an event conditioned on data is just another
posterior expectation, this time of an indicator variable.

\begin{align}
\textrm{Pr}[A \mid y]
&= \mathbb{E}[\textrm{I}[\Theta \in A] \mid y]
\\[6pt]
&= \int_{\Theta} \textrm{I}(\theta \in A) \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}

The expression $\textrm{I}(\theta \in A)$ takes on value 1 if $\theta
\in A$ and value 0 otherwise.  We write $\textrm{I}[\Theta \in A]$
using square brackets because a random variable is a function, whereas
we write $\textrm{I}(\theta \in A)$ because $\theta \in \mathbb{R}^D$
is a value.


### Posterior predictive inference

Often we are interested in predicting new data $\tilde{y}$ given the
observation of existing data $y$.  This is a form of _posterior
predictive inference_.  For example, $y$ might be the price of a stock
over some time period and $\tilde{y}$ the price of the stock in the
future.  Or $y$ might be the result of past games and $\tilde{y}$ the
winner of tomorrow's game.  Posterior predictive inference is also
cast an expectation, this time of a density.

\begin{align}
p(\tilde{y} \mid y)
&= \mathbb{E}\!\left[ \, p(\tilde{y} \mid \Theta) \mid y \, \right]
\\[6pt]
&= \int_{\Theta} p(\tilde{y} \mid \theta) \cdot p(\theta \mid y) \, \textrm{d}\theta.
\end{align}



## Markov chain Monte Carlo

Stan performs asymptotically exact, full Bayesian inference using
Markov chain Monte Carlo (MCMC).  MCMC starts from an initial value
$\theta^{(0)}$, which can be random, user provided, or a mixture of
both, and then generates a sequence of values
$\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(m)}, \ldots$ forming a _Markov chain_.  
A sequence forms a Markov chain if each value is generated conditioned on
only the previous value, so that

$$
\theta^{(m + 1)} \sim q(\theta \mid \theta^{(m)}.
$$

Because of this dependency, the sequence of random variables making up
the Markov chain exhibit a degree of _autocorrelation_, meaning that
draw $\theta^{(m)}$ is correlated with draw $\theta^{(m+1)}$.

We typically assume some mild conditions on the Markov chain
transition.  First, it must be _ergodic_ in the sense of never getting
stuck in such a way it can't visit the rest of the parameter space.
Second, it must be _aperiodic_ in the sense of not cycling regularly
through regions of the parameter space.  Third, it needs to preserve
the target density in the sense that if  $\theta^{(m)} \sim
p(\theta \mid y)$ has the posterior as its marginal distribution, then
$\theta^{(m+1)} \sim p(\theta \mid y)$ also follows the posterior
distribution.  

If the initial value $\theta^{(0)}$ is drawn from the posterior,
$$
\theta^{(0)} \sim p(\theta \mid y),
$$
then each value in the Markov chain will be marginally distributed
according to the posterior,
$$
\theta^{(1)}, \ldots, \theta^{(M)}
\sim p(\theta \mid y).
$$

Typically we cannot draw an initialization from the posterior---if we
could, we'd just use simpler Monte Carlo methods without the Markov
chain dependencies.  In the situation where $\theta^{(0)}$ is not a
draw from the posterior $p(\theta \mid y)$, then as the chain
progresses, it will converge to the right distribution in the sense
that in the limit as $m \rightarrow \infty$, the draws approach the
corect distribution $\theta^{(m)} \sim p(\theta \mid y)$. 

Now suppose we have a posterior sample of draws $\theta^{(1)}, \ldots,
\theta^{(M)}$. We can estimate expectations by simply plugging in
values of $\theta^{(m)}$ and averaging.  If our chain satisfies the
mild conditions above, we know that we get the right answer asymptotically,

\begin{align}
\mathbb{E}\!\left[f(\theta) \mid y \right]
&= \int_{\Theta} f(\theta) \cdot p(\theta \mid y) \, \textrm{d}\theta
\\[6pt]
&= \lim_{M \rightarrow \infty} \ \frac{1}{M} \sum_{m=1}^M f\!\left( \theta^{(m)} \right).
\end{align}.

With only finite time in practice, we use the initial segment of the
chain to estimate expectations,
$$
\mathbb{E}\!\left[f(\theta) \mid y\right]
\approx \frac{1}{M} \sum_{m=1}^M f(\theta^{(m)}).
$$

If our initial draw is from the posterior, this estimate is unbiased.
In the usual situation, where the initial draw is not from the
posterior, our expectation retains a degree of bias. Nevertheless, the
limit above shows that the bias goes to zero in the limit. In
practice, we typically remove an initial segment of $N$ draws before
the chain has approximately converged to the right distribution, and
average the remaining draws,
$$
\mathbb{E}\!\left[f(\theta) \mid y\right]
\approx \frac{1}{M - N} \sum_{m=N + 1}^M f(\theta^{(m)}).
$$

Given an estimate $\widehat{\theta}$, its _error_ is $\widehat{\theta}
- \theta$. If its expected error is zero, an estimator is said to be
_unbiased_.   If the Monte Carlo draws $\theta^{(1)}, \ldots, \theta^{(M)}$ are
independent, the _central limit theorem_ tells us that our error
follows a normal distribution asymptotically, so that as $M \rightarrow \infty$,
the error of our estimate $\widehat{\theta}$ follows a normal distribution,
$$
\widehat{\theta} - \theta
\sim \textrm{normal}\!\left(0, \frac{\sigma}{\sqrt{M}}\right),
$$
where $\sigma$ is the standard deviation of the variable $\theta$.
Pre-asymptotically, this result holds approximately and will be used
to estimate the distribution of estimation errors.

Because draws in MCMC can be correlated (or even anti-correlated), we
need the MCMC CLT to generalize the central limit theorem to the case
of correlated variables. Here, we can estimate an _effective sample
size_ (ESS), which is the number of independent draws that provide the same
error distribution.  If $M^{\textrm{eff}}$ is the effective sample size of the
Markov chain $\theta^{(1)}, \ldots, \theta^{(M)}$, then the error will
be distributed approximately as

$$
\widehat{\theta} - \theta
\sim \textrm{normal}\!\left(0, \frac{\sigma}{\sqrt{M^{\textrm{eff}}}}\right).
$$




# References